<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ChenJing's Blog</title>
    <description>记录 | 陈晶，Web &amp; Mobile Lover，Algrithm Engineer | 这里是 @chenjing 的个人博客，与你一起发现更大的世界。</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Tue, 05 Nov 2019 12:45:20 +0800</pubDate>
    <lastBuildDate>Tue, 05 Nov 2019 12:45:20 +0800</lastBuildDate>
    <generator>Jekyll v3.8.6</generator>
    
      <item>
        <title>Stanford CS224n 自然语言处理（三）</title>
        <description>&lt;h1 id=&quot;0-前言&quot;&gt;0 前言&lt;/h1&gt;

&lt;p&gt;这篇文章涵盖 CS224n Lecture 2 中的部分内容。主要内容包括：word2vec 的目标函数梯度的计算；实现 word2vec 的两种模型：Skip-Gram 和 Continuous Bag-of-Word；训练方法：负采样 (Negative sampling)和 Hierarchical softmax。&lt;/p&gt;

&lt;p&gt;我们回顾一下 word2vec，word2vec 模型实际上包含：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;两个算法（模型）&lt;/strong&gt;：continuous bag-of-words（CBOW）和 skip-gram。CBOW 是根据上下文单词来预测中心词。skip-gram 则相反，是根据中心词预测上下文词。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;两种训练方法：&lt;/strong&gt;negative sampling 和 hierarchical softmax。Negative sampling 通过抽取负样本来定义目标，hierarchical softmax 通过使用一个有效的树结构来计算所有词的概率来定义目标。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;1-continuous-bag-of-word-model&quot;&gt;1 Continuous Bag-of-Word Model&lt;/h1&gt;

&lt;h2 id=&quot;11-one-word-context&quot;&gt;1.1 One-word context&lt;/h2&gt;

&lt;p&gt;同样，我们首先从简单的例子开始。其实，one-word context 就是 3.2 中任务实施的示例，在这一节，我们不再举具体的示例，而是通过&lt;strong&gt;数学公式&lt;/strong&gt;来表征这一过程。我们规定，如若没有特殊说明，向量皆指&lt;strong&gt;列向量&lt;/strong&gt;。&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;⚠️警告，接下来会出现大篇幅的数学公式推导，若有看不明白的地方，请自己举一些具体的例子，或者参考 Stanford CS224n 自然语言处理（二） 3.2 中的任务实施示例&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cs224n/one_word_net_arch_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图是 CBOW 的最简化模型（实际上也是 Skip-Gram 的最简化模型），输入只有一个上下文词，且只预测一个目标词（即中心词）。 $\mathbf{W} \in \mathbb{R}^{V \times N}$ 是输入层到隐藏层的权重矩阵，$\mathbf{W}^{\prime} \in \mathbb{R}^{N \times V}$ 是隐藏层到输出层的权重矩阵，$\mathbf{x}$ 是输入 one-hot 向量，$\mathbf{h}$ 是隐藏向量，$\mathbf{y}$ 是输出概率向量， ${x_{1}, \cdots, x_{V}}$ ， ${h_{1}, \cdots, h_{N}}$ ， ${y_{1}, \cdots, y_{V}}$ ，分别代表输入层、隐藏层、输出层的神经元（units），其中 ${x_{1}, \cdots, x_{V}}$ ，中只有一个值为 1，其余值都为 0，我们假设 $x_{k}=1$，$x_{k^{\prime}}=0$， ${k^{\prime} \neq k}$。 $\mathbf{v}_w^{\mathrm{T}}$ 为 $\mathbf{W}^{\mathrm{T}}$ 的某一列，也就是说，$\mathbf{v}_w$ 为 $\mathbf{W}$ 的某一行，这里行列转换最好画个简单的矩阵辅助理解。我们有：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{h}=\mathbf{W}^{\mathrm{T}} \mathbf{x}=\mathbf{W}_{(k, \cdot)}^{ \quad \mathrm{T}} :=\mathbf{v}_{w_{I}}^{\mathrm{T}}  \tag{1}&lt;/script&gt;

&lt;p&gt;其中$\mathbf{v}&lt;em&gt;{w&lt;/em&gt;{I}}^{\mathrm{T}}$是个列向量，为$\mathbf{W}^{\mathrm{T}}$第 $k$ 列 ，形式上等于 $\mathbf{W}$ 的第 $k$ 行，这里的 $\mathbf{W}&lt;em&gt;{(k, \cdot)}^{\quad \mathrm{\mathrm{T}}}$ 的意思是先取 $\mathbf{W}$ 的第 $k$ 行，再转置 ，同样地， $\mathbf{h}$ 是个列向量，为 $\mathbf{W}^{\mathrm{T}}$ 第 $k$ 列 ，形式上等于 $\mathbf{W}$ 的第 $k$ 行。 $\mathbf{v}&lt;/em&gt;{w_{I}}$ 是输入词 ${w_{I}}$ 的向量表示，即词向量。这样解释不知大家明白否。&lt;/p&gt;

&lt;p&gt;其实这里暗示了从输入层隐藏层的激活函数是线性的，因为从 $\mathbf{v}&lt;em&gt;{w&lt;/em&gt;{I}}$ 到 $\mathbf{h}$ 只经过一个转置。&lt;/p&gt;

&lt;p&gt;从隐藏层到输出层有一个不同的权重矩阵 $\mathbf{W}^{\prime} \in \mathbb{R}^{N \times V}$ ，通过这个矩阵，对词表中的每一个词，我们最终能计算得到一个分数 $u_{j}，j = 1,2,\cdots,V$。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;u_{j}=\mathbf{v}_{w_{j}}^{\prime \mathrm{T}} \mathbf{h}  \tag{2}&lt;/script&gt;

&lt;p&gt;（2）中，$\mathbf{v}&lt;em&gt;{w&lt;/em&gt;{j}}^{\prime}$ 是 $\mathbf{W}^{\prime}$ 的第 $j$ 列，然后我们使用 softmax 函数将分数转化成后验概率。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p\left(w_{j} \mid w_{I}\right)=y_{j}= softmax(u_j)=\dfrac{\exp \left(u_{j}\right)}{\sum_{j^{\prime}=1}^{V} \exp \left(u_{j^{\prime}}\right)}  \tag{3}&lt;/script&gt;

&lt;p&gt;（3）中，$y_{j}$ 是输出层第 $j$ 个单元的输出，将（1）（2）带入（3）得到：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p\left(w_{j} \mid w_{I}\right)=\dfrac{\exp \left(\mathbf{v}_{w_{j}}^{\prime \ \mathrm{T}} \mathbf{v}_{w_{I}}\right)}{\sum_{j^{\prime}=1}^{V} \exp \left(\mathbf{v}_{w_{j^{\prime}}}^{\prime  \ {\mathrm{T}}} {\mathbf{v}_{w_{I}}}\right)}  \tag{4}&lt;/script&gt;

&lt;p&gt;通过以上几个式子，我们发现，似乎 $\mathbf{v}_w^{\prime}$ 也可以作为词向量，因为输出的理想状态也是一个 one-hot 向量。因此，$\mathbf{v}_w$ 与 $\mathbf{v}_w^{\prime}$ 是同一个词 $w$ 的两种词向量表示（前者称为&lt;strong&gt;输入词向量&lt;/strong&gt;，后者称为&lt;strong&gt;输出词向量&lt;/strong&gt;）。又因为 $\mathbf{v}_w$ 来自于 $\mathbf{W}$ 的行，$\mathbf{v}_w^{\prime}$ 来自于 $\mathbf{W}^{\prime}$ 的列，所以，我们称 $\mathbf{W}$ 为&lt;strong&gt;输入词向量矩阵&lt;/strong&gt;，$\mathbf{W}^{\prime}$ 为&lt;strong&gt;输出词向量矩阵&lt;/strong&gt;。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;在之后的论述中，我们将 $\mathbf{v}&lt;em&gt;w$ 与 $\mathbf{v}&lt;/em&gt;{w}^{\mathrm{T}}$ 都称为输入词向量，只不过前者是行向量，后者是列向量。&lt;/p&gt;

  &lt;p&gt;将 $\mathbf{v}_w^{\prime}$ 与 $\mathbf{v}_w^{\prime \; \mathrm{T}}$ 都称为输出词向量，只不过前者是列向量，后者是行向量。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;111-更新-mathbfwprime&quot;&gt;1.1.1 更新 $\mathbf{W}^{\prime}$&lt;/h3&gt;

&lt;p&gt;接下来，我们推导该模型权重更新的式子。 虽然实际中应用这样的式子来更新权重不具备可操作性（之后会解释），但这样的推导有利于我们深入理解原始模型而不应用任何技巧。&lt;/p&gt;

&lt;p&gt;对于一个训练样本，我们的训练目标是最大化（4）​，即&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} \max p\left(w_{O} \mid w_{I}\right) \tag{5} &amp;=\max y_{j^{*}} \\ &amp;=\max \log y_{j^{*}} \tag{6} \\ &amp;=u_{j^{*}}-\log \sum_{j^{\prime}=1}^{V} \exp \left(u_{j^{\prime}}\right) :=-E \tag{7} \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;$w_{O}$ 为实际的输出词（output word），我们记 $j^&lt;em&gt;$ 为 $w_{O}$ 在输出层的索引。 $E=-\log p\left(w_{O} \mid w_{I}\right)=\log \sum_{j^{\prime}=1}^{V} \exp \left(u_{j^{\prime}}\right)-u_{j^{&lt;/em&gt;}}$ 是我们的损失函数，和（7）相差一个负号，所以我们需要最小化 $E$ ， 这个损失函数是交叉熵损失的一种特殊情况，单个样本的二元交叉熵损失如下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L=-[y \log \hat{y}+(1-y) \log (1-\hat{y})]&lt;/script&gt;

&lt;p&gt;$y$ 是真实标签，$\hat{y}$ 是预测值，当 $y=1$ 时，上式变为 $L=-\log \hat{y}$ ，形式上和我们的损失函数一致。&lt;/p&gt;

&lt;p&gt;现在我们来推导从隐藏层到输出层的权重更新公式。我们从损失函数出发，求 $E$ 关于 $u_{j}$ （从隐藏层输入到输出层的第 $j$ 个单元的分数）的偏导数，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\dfrac{\partial E}{\partial u_{j}}=y_{j}-t_{j} :=e_{j} \tag{8}&lt;/script&gt;

&lt;p&gt;其中，$t_{j}=\mathbf{1}\left(j=j^{*}\right)$ ，意思是 $t_{j}$ 只有在第 $j$ 个单元为实际的输出词时才等于1，否则为0，也就是说 $\mathbf{t}=(t_{1} \; t_{2}  \cdots t_{V})^{\mathrm{T}}$ 是一个 one-hot  向量。注意，（8）仅仅是输出层的预测误差。&lt;/p&gt;

&lt;p&gt;然后，求求 $E$ 关于 $w_{ij}^{\prime}$ 的偏导数，也就得到了从隐藏层到输出层的权重更新的梯度。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\dfrac{\partial E}{\partial w_{i j}^{\prime}}=\dfrac{\partial E}{\partial u_{j}} \cdot \dfrac{\partial u_{j}}{\partial w_{i j}^{\prime}}=e_{j} \cdot h_{i} \tag{9}&lt;/script&gt;

&lt;p&gt;所以，当使用随机梯度下降（stochastic gradient descent）时，我们得到更新公式：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_{i j}^{\prime \; (\text{new})}=w_{i j}^{\prime \; (\text{old})}-\eta \cdot e_{j} \cdot h_{i} \tag{10}&lt;/script&gt;

&lt;p&gt;或者，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{v}_{w_{j}}^{\prime \; (\text{new})}=\mathbf{v}_{w_{j}}^{\prime \; (\text{old})}-\eta \cdot e_{j} \cdot \mathbf{h} \quad \text { for } j=1,2, \cdots, V \tag{11}&lt;/script&gt;

&lt;p&gt;其中，$\eta&amp;gt;0$ 是学习率，$e_{j}=y_{j}-t_{j}$ ， $h_{i}$ 是隐藏层的第 $i$ 个单元， $\mathbf{v}&lt;em&gt;{w&lt;/em&gt;{j}}^{\prime }$ 是 $w_j$ 的输出词向量。这个更新公式意味着我们需要遍历整个词汇表，检查每个单词的输出概率 $y_j$ ，并与我们期望的输出 $t_j$ 相比较。&lt;/p&gt;

&lt;p&gt;从（11）中还可以看出，如果 $y_j&amp;gt;t_j$ ，我们就从 $\mathbf{v}&lt;em&gt;{w&lt;/em&gt;{j}}^{\prime }$ 中减去隐藏向量 $\mathbf{h}$ （在这里，指的就是 $\mathbf{v}&lt;em&gt;{w&lt;/em&gt;{I}}$）的一部分，使得 $\mathbf{v}&lt;em&gt;{w&lt;/em&gt;{j}}^{\prime }$ 远离 $\mathbf{v}&lt;em&gt;{w&lt;/em&gt;{I}}$。如果 $y_j&amp;lt;t_j$ （只会在 $t_{j}=1$ 时发生，即 $w_{j}=w_{O}$） ，我们就给 $\mathbf{v}&lt;em&gt;{w&lt;/em&gt;{j}}^{\prime }$ 中加上隐藏向量 $\mathbf{h}$ 的一部分，使得 $\mathbf{v}&lt;em&gt;{w&lt;/em&gt;{O}}^{\prime }$ 接近 $\mathbf{v}&lt;em&gt;{w&lt;/em&gt;{I}}$。如果 $y_j$ 与 $t_j$ 非常接近，那么基本不更新，一次迭代训练完毕。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;这里的“接近”和“远离”，指的是两个词向量的内积越大越接近，内积越小越远离。其实很好理解，两个相同的词向量的内积最大，此时，这两个词向量完全相同，也就是最“接近”。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;112-更新-mathbfw&quot;&gt;1.1.2 更新 $\mathbf{W}$&lt;/h3&gt;

&lt;p&gt;我们得到了 $\mathbf{W}^{\prime}$ 更新式之后，现在的目标就是求得 $\mathbf{W}$ 的更新式。现在，我们求 $E$ 关于 $h_{i}$ 的偏导数，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\dfrac{\partial E}{\partial h_{i}}=\sum_{j=1}^{V} \dfrac{\partial E}{\partial u_{j}} \cdot \dfrac{\partial u_{j}}{\partial h_{i}}=\sum_{j=1}^{V} e_{j} \cdot w_{i j}^{\prime} :=\mathrm{EH}_{i} \tag{12}&lt;/script&gt;

&lt;p&gt;其中，$h_{i}$ 是隐藏层的第 $i$ 个单元， $u_{j}$、 $e_{j}$ 与 1.1.1 中的定义一致， $EH$ 是一个 $n$ 维向量。&lt;/p&gt;

&lt;p&gt;我们回顾一下隐藏层做的线性计算操作，根据（1），我们可以知道，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_{i}=\sum_{k=1}^{V} x_{k} \cdot w_{k i} \tag{13}&lt;/script&gt;

&lt;p&gt;然后我们求 $E$ 关于 $\mathbf{W}$ 每一个元素 $w_{kj}$ 的偏导数，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\dfrac{\partial E}{\partial w_{k i}}=\dfrac{\partial E}{\partial h_{i}} \cdot \dfrac{\partial h_{i}}{\partial w_{k i}}=\mathrm{EH}_{i} \cdot x_{k} \tag{14}&lt;/script&gt;

&lt;p&gt;所以我们得到，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\dfrac{\partial E}{\partial \mathbf{W}}=\mathbf{x} \otimes \mathbf{E H}=\mathbf{x} \mathrm{EH}^{\mathrm{T}} \tag{15}&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;$\mathbf{·}$ 表示内积（inner product）， $\otimes$ 表示外积（tensor product）。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;最终我们获得了一个 $V \times N$ 的矩阵，因为  $\mathbf{x}$ 中只有一个元素不为 0，所以，在 $ \dfrac{\partial E}{\partial \mathbf{W}} $ 中只有一行为非零行，这一行即为 $\mathrm{EH}^{\mathrm{T}}$，是一个 $n$ 维行向量。最终的更新公式为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{v}_{w_{I}}^{(\text{new})}=\mathbf{v}_{w_{I}}^{(\text{old})}-\eta \cdot \mathrm{EH}^{\mathrm{T}} \tag{16}&lt;/script&gt;

&lt;p&gt;其中，$\mathbf{v}&lt;em&gt;{w_I}$ 是 $\mathbf{W}$ 的行向量，所以 $\mathbf{v}&lt;/em&gt;{w_I}^{\mathrm{T}}$ 也是上下文词的输入词向量，在上下文词只有一个的情况下，在一次迭代中这是唯一导数不为 0 的行。$\mathbf{W}$ 的其他行在这次迭代中保持不变，因为他们的导数都是 0。&lt;/p&gt;

&lt;h2 id=&quot;12-multi-word-context&quot;&gt;1.2 Multi-word context&lt;/h2&gt;

&lt;p&gt;下面我们扩展一下上面介绍的模型结构，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cs224n/CBOW_arc.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这里我们认为有多个上下文词，假设有 $C$ 个，来预测一个目标词（即中心词）。和 1.1 中介绍的类似，我们将每个上下文词与 $\mathbf{W}^{\mathrm{T}}$ 做矩阵乘法，然后对这 $C$ 个结果取平均，作为中间层的隐藏向量。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} \mathbf{h} &amp;= \dfrac{1}{C} \mathbf{W}^{\mathrm{T}} (\mathbf{x}_1+\mathbf{x}_2+ \cdots + \mathbf{x}_C) \tag{17} \\ &amp;= \dfrac{1}{C} (\mathbf{v}_{w_{1}}+\mathbf{v}_{w_{2}}+\cdots+\mathbf{v}_{w_{C}})^{\mathrm{T}}  \tag{18}\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;同样这里的 $\mathbf{v}_{w}$ 是输入词向量，$w_1, w_2, …, w_C$ 是上下文词，所以损失函数为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} E&amp;=-\log p\left(w_{O} \mid w_{I,1},w_{I,2},\cdots,w_{I,C}\right)\tag{19}\\ &amp;=-u_{j^{*}} + \log \sum_{j^{\prime}=1}^{V} \exp \left(u_{j^{\prime}}\right)\tag{20}\\ &amp;= -\mathbf{v}_{w_{O}}^{\prime \mathrm{T}} \mathbf{h} + \log \sum_{j^{\prime}=1}^{V} \exp \left(\mathbf{v}_{w_{j}}^{\prime \mathrm{T}} \mathbf{h}\right)\tag{21}\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;式（21）与式（7）本质相同，因为今后我们基本不会遇到上下文词只有一个的情况，所以，CBOW 的损失函数可确定为式（21）。&lt;/p&gt;

&lt;h3 id=&quot;121-更新-mathbfwprime&quot;&gt;1.2.1 更新 $\mathbf{W}^{\prime}$&lt;/h3&gt;

&lt;p&gt;与 1.1.1 中的推导过程相同，从隐藏层到输出层的权重更新式为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{v}_{w_{j}}^{\prime \; (\text{new})}=\mathbf{v}_{w_{j}}^{\prime \; (\text{old})}-\eta \cdot e_{j} \cdot \mathbf{h} \quad \text { for } j=1,2, \cdots, V \tag{22}&lt;/script&gt;

&lt;p&gt;注意，对于每一个训练实例，都需要应用这个更新式来更新 $\mathbf{W}^{\prime}$ 中的每一个元素。&lt;/p&gt;

&lt;h3 id=&quot;122-更新-mathbfw&quot;&gt;1.2.2 更新 $\mathbf{W}$&lt;/h3&gt;

&lt;p&gt;与 1.1.2 中的推导过程稍微有些不同，从隐藏层到输出层的权重更新式为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{v}_{w_{I,c}}^{(\text{new})}=\mathbf{v}_{w_{I,c}}^{(\text{old})}-\dfrac{1}{C}\cdot\eta \cdot \mathrm{EH}^{\mathrm{T}} \quad \text { for } c=1,2, \cdots, C \tag{23}&lt;/script&gt;

&lt;p&gt;简单地推导一下：&lt;/p&gt;

&lt;p&gt;$\text{for  c = 1, 2,…, C}:$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\dfrac{\partial E}{\partial h_{i}}=\sum_{j=1}^{V} \dfrac{\partial E}{\partial u_{j}} \cdot \dfrac{\partial u_{j}}{\partial h_{i}}=\sum_{j=1}^{V} e_{j} \cdot w_{i j}^{\prime} :=\mathrm{EH}_{i}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_{i}=\dfrac{1}{C} \sum_{c=1}^C\sum_{k=1}^{V} x_{ck} \cdot w_{k i}^{c}&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;$h_i$  的表达式与 1.1.2 中的不同&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\dfrac{\partial E}{\partial w_{k i}^c}=\dfrac{\partial E}{\partial h_{i}} \cdot \dfrac{\partial h_{i}}{\partial w_{k i}^c}=\mathrm{EH}_{i} \cdot \dfrac{1}{C} \cdot x_{ck}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\dfrac{\partial E}{\partial \mathbf{W}^c}=\dfrac{1}{C}\mathbf{x}_c \otimes \mathbf{E H}=\dfrac{1}{C}\mathbf{x}_c \mathrm{EH}^{\mathrm{T}}&lt;/script&gt;

&lt;p&gt;最终我们获得了 $C$ 个 $V \times N$ 的矩阵，因为  $\mathbf{x}$ 中只有一个元素不为 0，所以，在 $ \dfrac{\partial E}{\partial \mathbf{W}^c} $ 中只有一行为非零行，这一行即为 $\mathrm{EH}^{\mathrm{T}}$，是一个 $n$ 维行向量。&lt;/p&gt;

&lt;p&gt;容易看出，从输入层到隐藏层的权重更新式与式（16）相似，只不过我们需要将式（23）应用于更新每一个上下文词 $w_{I,c}$。式（23）中，$\mathbf{v}&lt;em&gt;{w&lt;/em&gt;{I,c}}$ 表示第 $c$ 个输入上下文词中的词向量，$\eta$ 为正的学习率，$\mathrm{EH} = \dfrac{\partial E}{\partial h_{i}}$ 由式（12）给出。其中，$\mathbf{v}&lt;em&gt;{w_I,c}$ 是 $\mathbf{W^c}$ 的行向量，所以 $\mathbf{v}&lt;/em&gt;{w_{I,c}}^{\mathrm{T}}$也是上下文词的输入词向量，在上下文词只有一个的情况下，在一次迭代中这是唯一导数不为 0 的行。$\mathbf{W^c}$ 的其他行在这次迭代中保持不变，因为他们的导数都是 0。&lt;/p&gt;

&lt;h2 id=&quot;2-skip-gram-model&quot;&gt;2 Skip-Gram Model&lt;/h2&gt;

&lt;p&gt;Skip-Gram 与 CBOW 相反，输入是中心词，输出是上下文词。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Skip-Gram 是 一 预测 多。&lt;/p&gt;

  &lt;p&gt;CBOW 是 多 预测 一。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cs224n/SG_arc.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我们仍然使用 $\mathbf{v}_{w_I}$ 表示输入层中心词的输入词向量，因此隐藏层状态 $h$ 的定义与式（1）相同。也即意味着 $h$ 只是复制（转置）输入层 $\to$ 隐藏层 权重矩阵 $\mathbf{W}$ 的一行，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{h}=\mathbf{W}^{\mathrm{T}} \mathbf{x}=\mathbf{W}_{(k, \cdot)}^{ \quad \mathrm{T}} :=\mathbf{v}_{w_{I}}^{\mathrm{T}}  \tag{24}&lt;/script&gt;

&lt;p&gt;在输出层，我们将输出 $C$ 个概率分布，而不是一个。 使用相同的 隐藏层 $\to$ 输出层 矩阵计算每个输出：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p\left(w_{c,j}=w_{O,c} \mid w_{I}\right)=y_{c, j}= softmax(u_{c,j})=\dfrac{\exp \left(u_{c,j}\right)}{\sum_{j^{\prime}=1}^{V} \exp \left(u_{j^{\prime}}\right)}  \tag{25}&lt;/script&gt;

&lt;p&gt;其中，$w_{c,j}$ 是第 $c$ 个 $\mathbf{W}^{\prime}$ 中的第 $j$ 列的输出词向量所代表的词； $w_{O,c}$ 输出上下文词中的第 $c$ 个词； $w_{I}$ 是输入词； $y_{c,j}$ 是第 $c$ 个 $\mathbf{W}^{\prime}$ 中的第 $j$ 个单元的输出； $u_{c,j}$ 第 $c$ 个 $\mathbf{W}^{\prime}$ 的输出中第 $j$ 个分数；因为输出权重矩阵共享参数，因此：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;u_{c,j}=u_{j}=\mathbf{v}_{w_{j}}^{\prime \quad \mathrm{T}} \cdot \mathbf{h,} \quad \text{for c = 1, 2, ... , C} \tag{26}&lt;/script&gt;

&lt;p&gt;其中，$\mathbf{v}&lt;em&gt;{w&lt;/em&gt;{j}}^{\prime}$ 是第 $j$ 个输出词向量对应的词，也是 $\mathbf{W}^{\prime}$ 第 $j$ 列。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;这里提一句，模型初始化时，生成相同的 C 个分布，但是在反向传播时，由于每个词不同，各自减去的比例也不同，因此 $\mathbf{W}^{\prime}$ 也会根据不同的词发生不同的变化。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;21-更新-mathbfwprime&quot;&gt;2.1 更新 $\mathbf{W}^{\prime}$&lt;/h2&gt;

&lt;p&gt;隐藏层 $\to$ 输出层的参数更新方程的推导与 1.1 中的没有太大不同。损失函数改为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} E&amp;=-\log p\left(w_{O} \mid w_{I,1},w_{I,2},\cdots,w_{I,C}\right)\tag{27}\\ &amp;= -\log[p\left(w_{O,1} \mid w_{I}  \right) p\left(w_{O,2} \mid w_{I} \right) \cdots p\left(w_{O,C} \mid w_{I}  \right)] \tag{28} \\ &amp;=-\log \prod_{c=1}^{C} \frac{\exp \left(u_{c, j_{c}^{*}}\right)}{\sum_{j^{\prime}=1}^{V} \exp \left(u_{j^{\prime}}\right)}\tag{29}\\ &amp;= -\log [\prod_{c=1}^{C} \exp \left(u_{c, j_{c}^{*}}\right)] + -\log [\prod_{c=1}^{C} \sum_{j^{\prime}=1}^{V} \exp \left(u_{ j^{\prime}}\right)] \tag{30} \\ &amp;= -\sum_{c=1}^{C} u_{j_{c}^{*}}+C \cdot \log \sum_{j^{\prime}=1}^{V} \exp \left(u_{j^{\prime}}\right)\tag{31}\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;其中， $u_{j_{c}^{*}}$ 是第 $c$ 个输出上下文词的索引。&lt;/p&gt;

&lt;p&gt;我们求输出层的每个 panel 上的每个单元的输入 $u_{c,j}$ 对 $E$ 的偏导数，得到：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\dfrac{\partial E}{\partial u_{c, j}}=y_{c, j}-t_{c, j}:=e_{c, j} \tag{32}&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;这里的 panel 的意思是 “面板” ， 可以看 Skip-Gram 模型的输出部分加以理解。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;和（8）类似， $e_{c, j}$ 是预测误差。为了简化标记，我们定义一个 $V$ 维的向量 $\mathrm{EI}=\left{\mathrm{EI}&lt;em&gt;{1}, \cdots, \mathrm{EI}&lt;/em&gt;{V}\right}$ ，作为所有输出上下文词的预测误差的和。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathrm{EI}_{j}=\sum_{c=1}^{C} e_{c, j} \tag{33}&lt;/script&gt;

&lt;p&gt;然后，我们求 $\mathbf{W}^{\prime}$ 对 $E$ 的偏导数：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\dfrac{\partial E}{\partial w_{i j}^{\prime}}=\sum_{c=1}^{C} \dfrac{\partial E}{\partial u_{c, j}} \cdot \dfrac{\partial u_{c, j}}{\partial w_{i j}^{\prime}}=\mathrm{EI}_{j} \cdot h_{i} \tag{34}&lt;/script&gt;

&lt;p&gt;最终得到 $\mathbf{W}^{\prime}$ 的更新式：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_{i j}^{\prime \quad \mathrm{(new)}} = w_{i j}^{\prime \quad \mathrm{(old)}}-\eta \cdot \mathrm{EI}_{j} \cdot h_{i} \tag{35}&lt;/script&gt;

&lt;p&gt;或，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{v}_{w_{j}}^{\prime \quad \mathrm{(new)}} =\mathbf{v}_{w_{j}}^{\prime \quad \mathrm{(old)}}-\eta \cdot \mathrm{EI}_{j} \cdot \mathbf{h} \quad \text { for } j=1,2, \cdots, V \tag{36}&lt;/script&gt;

&lt;p&gt;除了预测误差是输出层中所有上下文词中累加而成的外，对更新方程（36）的直观理解与（11）相同。 注意，我们需要为每个训练实例的 hidden $\to$ output 矩阵的每个元素应用此更新方程。&lt;/p&gt;

&lt;h2 id=&quot;22-更新-mathbfw&quot;&gt;2.2 更新 $\mathbf{W}$&lt;/h2&gt;

&lt;p&gt;输入层 $\to$ 隐藏层矩阵的更新公式的推导与（12）至（16）相同，不同之处在于将预测误差 $e_j$ 替换为 $\mathrm{EI}_j$ 。 我们直接给出更新公式：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{v}_{w_{I}}^{(\text {new})}=\mathbf{v}_{w_{I}}^{(\text {old})}-\eta \cdot \mathrm{EH}^{\mathrm{T}} \tag{37}&lt;/script&gt;

&lt;p&gt;其中 $\mathrm{EH}$ 是一个 $N$ 维向量。由（12）：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\dfrac{\partial E}{\partial h_{i}}=\sum_{j=1}^{V} \sum_{c=1}^{C} \dfrac{\partial E}{\partial u_{c, j}} \cdot \dfrac{\partial u_{c, j}}{\partial h_{i}} = \sum_{j=1}^{V} \underbrace{\sum_{c=1}^{C} e_{c,j}}_{\mathrm{EI}_{j}} \cdot w_{ij}^{\prime} = \mathrm{EH}_i \tag{38}&lt;/script&gt;

&lt;p&gt;所以 $\mathrm{EH}$ 的每个元素为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathrm{EH}_i = \sum_{j=1}^{V} e_{c,j} \cdot w_{ij}^{\prime} \tag{39}&lt;/script&gt;

&lt;p&gt;对（37）的理解与（16）处的相同。&lt;/p&gt;

&lt;h1 id=&quot;3-小结&quot;&gt;3 小结&lt;/h1&gt;

&lt;p&gt;这篇文章论述了 word2vec 的两个模型 CBOW 和 Skip-Gram 的数学表示，包含详细的数学推导及验证，如果有难以理解的地方，请一定要举出具体的例子，比如，举出一个 $\mathbf{W}$ 和 $\mathbf{W}^{\prime}$ 去走一遍数学推导。由于篇幅不宜过长，两种可以提高训练效率的方法将在下一篇文章中详细说明。&lt;/p&gt;

&lt;h2 id=&quot;参考文献&quot;&gt;参考文献&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1411.2738.pdf&quot;&gt;word2vec Parameter Learning Explained &lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 05 Nov 2019 20:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/11/05/Stanford-CS224n-nlp-03/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/11/05/Stanford-CS224n-nlp-03/</guid>
        
        <category>NLP</category>
        
        <category>Stanford</category>
        
        <category>深度学习</category>
        
        
      </item>
    
      <item>
        <title>NLP算法工程师进击之路</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;路线是重要的，因为大部分人都走在大路上。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;其实很早就想写一篇文章里来讲讲自己一年以来的学习情况，以及为什么选择自然语言处理这个方向。第一部分是前期探索，第二部分是个人认为比较好的入门自然语言处理的学习路线，如果不想看前期探索，点&lt;a href=&quot;#build&quot;&gt;这里&lt;/a&gt;可以直接跳到第二部分。&lt;/p&gt;

&lt;h2 id=&quot;前期探索&quot;&gt;前期探索&lt;/h2&gt;

&lt;p&gt;我2018年9月入学成为一名研究生，到现在差不多满一年了。导师的研究方向是机器学习、算法相关，我本科没接触过算法，因此对算法这一块是一窍不通，大四临近毕业的时候，只想着玩了并没有想到提前补课，导致我远远落后于那些本科有算法背景的同级生。在研究生入学前的暑假，因为玩的也差不多了😷，就开始了解机器学习相关的东西，请教了同门的师兄师姐，决定先从 Python、《统计学习方法》和西瓜书看起。但是我的自控力很差，导致效率很低，一个暑假过去也就学了Python 和《统计学习方法》的部分内容，当时并没有感到紧迫性…现在真想抽自己几下。&lt;/p&gt;

&lt;p&gt;学期开始，有很多琐事，课程也比较繁重，第一个学期我只学了吴恩达在Coursera上的公开课，这个公开课比较基础，个人感觉对于机器学习方向的同学来说，并没有什么卵用。不如直接看他在斯坦福开的课程 CS229 2017。同时，我也在自学日语，用的教材是《标准日本语》，第一学期差不多学了初级上册。&lt;/p&gt;

&lt;p&gt;总结一下，本人第一学期学了 Coursera 上的机器学习公开课和标准日本语初级上。(当然这没有包括研究生课程，说实话研究生课程中《矩阵论》还是非常有用的)&lt;/p&gt;

&lt;p&gt;之后是研一的寒假，我没有学习。。&lt;/p&gt;

&lt;p&gt;研一第二学期，开始有种紧迫感了，所以买了一门有关深度学习的网课，花了半个月看完，然后就开始使用PyTorch 做一些小练习。4月份的时候导师让我们选方向，因为上一届的两个学长是做 NLP 的，听了他们分析有关 CV 和 NLP 两者未来前景，加之自己的思考之后，我选择了NLP方向。我选择的入门课程是斯坦福的 CS224n 2019，这是一门经典课程，主讲人 Manning 也是 NLP 领域的大牛。在学期末，我们差不多完成了这门课程，包括所有的课程作业。在这之后，我做了一个有关文本分类的新人赛，但是效果一般。值得一提的是，我在这个学期，还学完了标准日本语初级下册和中级上册。&lt;/p&gt;

&lt;p&gt;暑假开始，我学了斯坦福的另一门课程 CS224u 自然语言理解，他是自然语言处理的一大部分（另一部分是自然语言生成），比较细致的补充了在 CS224n 里面没有涉及的知识点。然后，心血来潮复习了机器学习的内容，是一个大佬在b站发的课程：机器学习-白板推导系列，个人觉得特别好，秒杀一众讲机器学习的网课。&lt;/p&gt;

&lt;p&gt;好了，学习历程大致就是如此，接下来就是一些建议了。&lt;/p&gt;

&lt;p id=&quot;build&quot;&gt;&lt;/p&gt;

&lt;h2 id=&quot;给后来人的建议&quot;&gt;给后来人的建议&lt;/h2&gt;

&lt;p&gt;说是给后来人的建议，其实是，如果再给我一次机会，自己会按照怎样的方式去学习。就是因为当初自己走了太多弯路，现在才想把效率最高的路线写出来。&lt;/p&gt;

&lt;p&gt;如果你是一个打算做nlp算法的准研究生，下面的路线是我觉得效率最高的，仅代表个人看法。&lt;/p&gt;

&lt;p&gt;在本科快结束的时候，做完毕业设计，一般会有一个月的空闲时间，如果不去毕业旅行，这一个月，推荐大家学习一个大佬在b站发的课程：机器学习-白板推导系列，（需要这门课程的同学可以看我另一篇&lt;a href=&quot;https://kgoeson.github.io/2019/08/16/machine-learning-01/&quot;&gt;博客&lt;/a&gt;），边听边记笔记，然后配合着李航老师的统计学习方法，大致的了解一下机器学习基础内容，当然微积分概率论线性代数知识是必备的，如果已经忘了的话，要去复习。&lt;/p&gt;

&lt;p&gt;到了暑假，应该会更空，如果提前去实验室（在家也行），就可以学习 Python 了，不过要记住，编程语言只是工具，重要的是利用工具做什么。熟悉Python的基础语法之后，就可以开始实践了，直接找一个小项目，例如泰坦尼克号数据集，应用一下 Python 及其第三方库：numpy，pandas，matlibplot，sklearn 等等。把一个项目做熟悉，代码都自己敲，有什么问题 Google + 百度。这个步骤只是熟悉Python和常用第三方库的 API。之后也可以把经典机器学习的算法用代码复现，但是工作量会比较大，不过有利于理解。我没有做这一步，个人也不推荐，有这个时间可以把西瓜书看了。同时，暑假还可以熟悉一下 Linux 的常用命令，因为以后免不了要和服务器打交道， Linux 的常用操作是必须的。&lt;/p&gt;

&lt;p&gt;开学之后，第一年是要上课的，尽量把难搞的课放在第一学期，这样第二学期的课会轻松很多，第一学期，边上课，边看算法第四版，没错，就是下图这本算法，整整400页，不看完这本都不好意思说自己是做算法的，然后刷leetcode，可以找一些刷教程，github上有很多的不错的分析。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/algorithm_4th_edition.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;寒假开始，你就可以开始正式入门深度学习，我没看过花书，深度学习的理论是来自于买的一门网课…网课的质量一般，不过也算入门了，然后选择一个深度学习的框架，推荐 PyTorch 和 TensorFlow，我学的是 PyTorch。去 PyTorch 的官方网站利用提供的终端命令下载(这个时候就体现出Linux命令的重要性了)，然后跟着官方的教程入门，熟悉常用的API。&lt;/p&gt;

&lt;p&gt;第二个学期开始，因为课很少，会有很多个人可支配时间，就可正式入门 NLP 了，推荐斯坦福的 CS224n 2019，需要这门课程的同学可以看我另一篇&lt;a href=&quot;https://kgoeson.github.io/2019/08/04/Stanford-CS224n-nlp-01/&quot;&gt;博客&lt;/a&gt;。看官网的课程时间表，每节课之后都有一些推荐阅读和附加阅读，全部读完，一定不能着急，一个星期上一节课都可以。一共有5次作业，每次都要认认真真做完，不会的可以Google + 百度。切记，一定不能着急，不要一味求快。CS224n 一共有20节课，5次普通作业，一次大作业（这些作业都是用 PyTorch 的），一个学期应该差不多。&lt;/p&gt;

&lt;p&gt;暑假开始，保质保量地经过以上的学习，你已经成为一个 NLP 初级研究者，接下来你就可以发挥自己的特长，做自己的研究了，既可以参加一个大比赛（例如天池的比赛），又可以发论文（idea + 实验），也可以找实习。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;总结一下：&lt;/strong&gt;（个人认为比较好的学习路线）&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;本科末尾：机器学习-白板推导系列 + 《统计学习方法》&lt;/li&gt;
  &lt;li&gt;本科和研究生之间的暑假：Python + 第三方库 + Linux + 西瓜书&lt;/li&gt;
  &lt;li&gt;研一第一学期：算法-第四版&lt;/li&gt;
  &lt;li&gt;研一寒假：深度学习及框架（PyTorch或TensorFlow）&lt;/li&gt;
  &lt;li&gt;研一第二学期：CS224n 2019&lt;/li&gt;
  &lt;li&gt;研一暑假：按照自己的想法&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;最后，欢迎你加入 NLP field。&lt;/p&gt;

&lt;hr /&gt;

</description>
        <pubDate>Thu, 22 Aug 2019 03:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/08/22/NLP_algorithm_engineer_attacking/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/08/22/NLP_algorithm_engineer_attacking/</guid>
        
        <category>算法</category>
        
        <category>机器学习</category>
        
        <category>深度学习</category>
        
        <category>NLP</category>
        
        
      </item>
    
      <item>
        <title>机器学习-白板推导系列（一）</title>
        <description>&lt;h1 id=&quot;绪论&quot;&gt;绪论&lt;/h1&gt;

&lt;p&gt;再开一个坑。&lt;/p&gt;

&lt;p&gt;前段时间突然想巩固机器学习的相关内容，幸而在b站看到一位大佬的&lt;a href=&quot;https://space.bilibili.com/97068901/video&quot;&gt;《机器学习-白板推导》系列课程&lt;/a&gt;，花了大概20天左右把课程听完了。这个系列的课程对 “小白”（学过线性代数、概率论以及微积分）非常友好，因为你面对的不是枯燥的、满是公式的专业教材，而是一个愿意花时间做出细致且形象的解释的大佬😏，而且大佬一步一步非常清晰地 纯 · 手推公式，大佬请受我一拜。大佬在讲一个模型时会引申出很多模型和算法，并找出他们的相同点和不同点，到后来你会发现很多模型都有内在的联系，豁然开朗，越听越有味，一天不听浑身发痒。。。希望你也有这样的体会😬&lt;/p&gt;

&lt;p&gt;好了，开这个系列是为了保存笔记，其间可能会加一些自己对模型或算法的理解，但大部分以大佬的为主。&lt;/p&gt;

&lt;p&gt;一开始，介绍了机器学习的两个派别，&lt;strong&gt;频率派&lt;/strong&gt;和&lt;strong&gt;贝叶斯派&lt;/strong&gt;，频率派渐渐发展出了&lt;strong&gt;统计机器学习&lt;/strong&gt;，贝叶斯派渐渐发展出了&lt;strong&gt;概率图模型&lt;/strong&gt;。&lt;/p&gt;

&lt;h3 id=&quot;参考书籍&quot;&gt;参考书籍：&lt;/h3&gt;

&lt;p&gt;李航的《统计学习方法》、周志华的《机器学习》（西瓜书）、《Deep Learning》（花书），以及机器学习三大神书《Pattern Recognition and Machine Learning》（PRML）、《Machine Learning : A Probabilistic Perspective》（MLAPP）、《The Elements of Statistical Learning》（ESL）。其中，李航的《统计学习方法》和《ESL》属于频率派；《PRML》属于贝叶斯派；《MLAPP》有点像《PRML》和《ESL》的结合体，是百科全书性质的书，但主要以贝叶斯的角度来写；周志华的机器学习更像是一本手册，没有深入的公式推导，但介绍的很全面；《Deep Learning》就是大名鼎鼎的花书了，看书名就知道是讲深度学习的。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;李航的《统计学习方法》中讲了10个算法，用一句口诀来记：感K朴决逻，支提E隐条。《PRML》的主要内容也可总结为一句口诀：回分神核稀，图混近采连，顺组。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;视频资料&quot;&gt;视频资料：&lt;/h3&gt;

&lt;p&gt;1  台湾大学林轩田的《机器学习基石》：VC Theory、正则化、线性模型等；《机器学习技法》：SVM、决策树、随机森林、神经网络等。&lt;/p&gt;

&lt;p&gt;2  张志华的《机器学习导论》：主要是以频率派的角度阐述；《统计机器学习》：主要讲统计上的一些理论，以贝叶斯的角度阐述，偏数学方面。这两门课是张志华老师在上海交通大学时开的，现在张志华老师已经去了北大。&lt;/p&gt;

&lt;p&gt;3  斯坦福大学 Andrew Ng（吴恩达）: Stanford CS229 2017，非常有名，不介绍了。&lt;/p&gt;

&lt;p&gt;4  悉尼科技大学徐亦达的《机器学习》：阐述一些列概率模型，EM、MCMC、Calman Filter，粒子滤波，狄利克雷过程。GitHub上有笔记，很全！&lt;/p&gt;

&lt;p&gt;5  台湾大学李宏毅的《机器学习》：CNN、DNN；《MLDS》：优化、正则化、实践优化、自然语言处理等。&lt;/p&gt;

&lt;h3 id=&quot;符号约定&quot;&gt;符号约定&lt;/h3&gt;

&lt;p&gt;我们先规定一些符号：$\mathbf{X}$ 表示数据（data），是一个样本矩阵，每一行表示一个样本（随机变量），$\theta$ 表示参数（parameter），多数情况下是一个向量。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathbf{X}=(X_1\ X_2\ \cdots \ X_N)^{\mathrm{T}}=\begin{pmatrix} X_1^{\mathrm{T}}\\ X_2^{\mathrm{T}} \\ \vdots \\ X_N^{\mathrm{T}} \\ \end{pmatrix}\\ = \begin{pmatrix} x_{11} &amp; x_{12} &amp; \cdots &amp;x_{1p}\\ x_{21} &amp; x_{22} &amp; \cdots &amp;x_{2p}\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ x_{N1}&amp; x_{N2} &amp; \cdots &amp; x_{Np} \\ \end{pmatrix}\\ %]]&gt;&lt;/script&gt;

&lt;p&gt;其中，$\mathbf{X} \in \mathbb{R}^{ N \times p}$， $X_i \in \mathbb{R}^{ p \times 1}, i = 1,2,\cdots,N$。我们用大写粗体的 $\mathbf{X}$ 表示由 $N$ 个随机变量组成的矩阵，用 大写细体的 $X$ 表示随机变量，用小写细体的 $x$ 表示随机变量的具体取值，$x$ 可以是标量或向量，都用相同类型的字母表示，除特别声明外，本书中的向量均为列向量，$x$ 的特征向量记作：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(x^{(1)}\ x^{(2)}\ \cdots \ x^{(n)})^{\mathrm{T}}&lt;/script&gt;

&lt;p&gt;$x^{(i)}$ 表示 $x$ 的第 $i$ 个特征，注意，$x^{(i)}$ 与 $x_i$ 不同，后者表示多个随机变量的第 $i$ 个取值，即，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(x_i^{(1)}\ x_i^{(2)}\ \cdots \ x_i^{(n)})^{\mathrm{T}}&lt;/script&gt;

&lt;p&gt;若 $X$ 服从于一个概率分布，记为 $ X\sim P(X \mid \theta)$，这里我们用大写的 $P(·)$ 表示概率分布，用小写的 $p(·)$ 表示概率密度函数或离散分布律。此处，当 $\theta$ 为参数时， 以下两种表示方式等价： $P(X \mid \theta) \iff P(X;\theta)$ 。今后若不特殊说明，我们都用左侧的表示方式。&lt;/p&gt;

&lt;p&gt;今后的符号都依照以上规则。&lt;/p&gt;

&lt;h3 id=&quot;频率派-vs-贝叶斯派&quot;&gt;频率派 VS 贝叶斯派&lt;/h3&gt;

&lt;h4 id=&quot;频率派&quot;&gt;频率派&lt;/h4&gt;

&lt;p&gt;$\theta$ 为未知常量，$X$ 为随机变量，在这里我们要估计的是 $\theta$。最常用的方法是极大似然估计（Maximum Likelihood Estimation），&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_{MLE}=\underset{\theta}{\operatorname{argmax}}\; \log  \underbrace{P(X \mid \theta)}_{L(\theta)}&lt;/script&gt;

&lt;p&gt;$L(\theta)$ 是似然函数，$L(\theta)=P(X \mid \theta)=\prod_{i=1}^N p(x_i \mid \theta) $， 加 $\log$ 是为了简化运算，利用对数的运算性质，将连乘变为连加，$\prod \to \sum$ ，即 $\log P(X \mid \theta)=\sum_{i=1}^N p(x_i \mid \theta)$。我们的目的是求一个 $\hat{\theta}$ 使得 $P(X \mid \hat{\theta})$ 最大。&lt;/p&gt;

&lt;p&gt;所以，以频率派的视角，最终要解决的是&lt;strong&gt;优化问题&lt;/strong&gt;。&lt;/p&gt;

&lt;h4 id=&quot;贝叶斯派&quot;&gt;贝叶斯派&lt;/h4&gt;

&lt;p&gt;$\theta$ 为随机变量，$\theta \sim p(\theta)$，是先验概率分布（prior）。在这种情况下，我们最终要求的是后验概率 $P(\theta \mid X)$，由贝叶斯公式可得：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\theta \mid X)= \dfrac{P(X \mid \theta)P(\theta)}{P(X)} \propto P(X \mid \theta)P(\theta)&lt;/script&gt;

&lt;p&gt;其中，$P(\theta \mid X)$ 为后验概率（posterior）也就是我们要求的，$P(X \mid \theta)$ 为似然（likelihood），$P(\theta)$ 为先验（prior），$P(X)$ 是 $P(X, \theta)$ 的边缘分布，依据边缘概率的求法， $P(X)=\int_{\theta} {P(X \mid \theta)P(\theta)} \,{\rm d}\theta$ 是可以算出来的，可以认为是一个常值。&lt;/p&gt;

&lt;p&gt;因此，我们使用最大后验概率（Maximum  A  Posteriori）：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_{MAP}=\underset{\theta}{\operatorname{argmax}}\; P(X \mid \theta)P(\theta)&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;贝叶斯估计：&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\theta \mid X)= \dfrac{P(X \mid \theta)P(\theta)}{\int_{\theta} {P(X \mid \theta)P(\theta)} \,{\rm d}\theta}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;贝叶斯预测：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;已知 $X$，预测 $\widetilde{X}$，首先，我们通过 $X$ 去预测 $\theta$，再通过 $\theta$ 预测 $\widetilde{X}$，因此：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\widetilde{X} \mid X)=\int_{\theta} {P(\widetilde{X},\theta \mid X)} \,{\rm d}\theta =\int_{\theta} {P(\widetilde{X} \mid \theta) \underbrace{P(\theta \mid X)}_{posterior}} \,{\rm d}\theta&lt;/script&gt;

&lt;p&gt;所以，以贝叶斯派的视角，最终要解决的是&lt;strong&gt;求积分问题&lt;/strong&gt;。&lt;/p&gt;

&lt;hr /&gt;

</description>
        <pubDate>Sat, 17 Aug 2019 01:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/08/17/machine-learning-01/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/08/17/machine-learning-01/</guid>
        
        <category>机器学习</category>
        
        <category>公式推导</category>
        
        
      </item>
    
      <item>
        <title>Stanford CS224n 自然语言处理（二）</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;You shall know a word by the company it keeps.						—— J. R. Firth&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;0-前言&quot;&gt;0 前言&lt;/h1&gt;

&lt;p&gt;这篇文章涵盖 CS224n 的 Lecture 1 和 Lecture 2 中的部分内容。抛却开头的课程介绍，我们直入正题。主要内容包括：自然语言以及词义；词向量；共现矩阵( co-occurrence matrix ) ；降维方法( SVD )；word2vec 概览；实现 word2vec 的两种模型：Skip-Gram 和 Continuous Bag-of-Word。&lt;/p&gt;

&lt;p&gt;因为是第一篇正式介绍内容的文章，概念有点多，中途遇到看不懂的地方可以先略过，可能在后面会有解释，再返回去看，就会串起来。&lt;/p&gt;

&lt;h1 id=&quot;1-自然语言处理介绍&quot;&gt;1 自然语言处理介绍&lt;/h1&gt;

&lt;h2 id=&quot;11-自然语言&quot;&gt;1.1 自然语言&lt;/h2&gt;

&lt;p&gt;什么是自然（人类）语言？狭义地来说，自然语言通常是指一种自然地随文化演化的语言，即我们所知的汉语、英语、日语等。广义地来说，自然语言还包括人造语言，比如世界语。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;此处的“人造语言” 世界语，区别于编程语言等人造语言。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;人类语言是一个专门用来表达意义（meaning）的系统，而不是由任何物理表现产生的。以这种方式来说，它与视觉或任何形式的机器学习任务都不同。也就是说，图像是一种物理表现，这很好理解，日常生活中的照片，画册，亦或是计算机中的 RGB 色图，就是图像的物理表现，脱离了这种传递信息的介质，图像便不能很好地传递信息。而自然语言可以脱离这种方式，以自身直接传递信息，一般来说，你只需要通过若干个单词的描述，就能够准确无误地表达意义。&lt;/p&gt;

&lt;p&gt;大多数单词只是一个超语言实体（extra-linguistic entity）的符号：单词是一种符号，该符号映射到一个想法或事物。这种映射也称作指代语义（Denotational semantics）。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;signifier ( symbol )  $\Longleftrightarrow$  signified ( idea or thing )&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;signifier:&lt;/strong&gt; a sign’s physical form (such as a sound, printed word, or image) as distinct from its meaning.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;signified:&lt;/strong&gt; the meaning or idea expressed by a sign, as distinct from the physical form in which it is expressed.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;举个例子。&lt;strong&gt;signifier ( symbol )  $\Rightarrow$  signified ( idea or thing )&lt;/strong&gt; ，就是在纸上写着“椅子”（符号），你就能联想到具体的1号椅子，2号椅子…。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;signifier ( symbol )  $\Leftarrow$  signified ( idea or thing )&lt;/strong&gt;，指着具体的椅子给你看，你能抽象出椅子这一记号。&lt;/p&gt;

&lt;p&gt;还有，这些语言的符号可以被编码成几种形式：声音、手势、文字等。然后通过&lt;em&gt;连续&lt;/em&gt;信号传输给大脑，大脑本身似乎也能以一种连续的方式编码这些信号。&lt;/p&gt;

&lt;p&gt;那么，我们现在已经知道我们要 &lt;em&gt;“处理”&lt;/em&gt; 的对象了，那么该如何处理呢？这就是 CS224n 这个系列的主题，接下去的一系列文章将会带你领略 &lt;strong&gt;自然语言处理&lt;/strong&gt; 这门融语言学、计算机科学、数学于一体的学科。&lt;/p&gt;

&lt;h2 id=&quot;12-自然语言处理的任务&quot;&gt;1.2 自然语言处理的任务&lt;/h2&gt;

&lt;p&gt;自然语言处理的 目标 是&lt;strong&gt;通过设计算法来使计算机能够“理解”语言，从而能够执行某些特定的任务&lt;/strong&gt;，而不同的任务的难度是不一样的：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;简单：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;拼写检查&lt;/li&gt;
  &lt;li&gt;关键词搜索&lt;/li&gt;
  &lt;li&gt;同义词查找&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;中等：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;解析来自网站文档等的信息&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;困难：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;机器翻译&lt;/li&gt;
  &lt;li&gt;语义分析（例如，“陈述”这个词是什么意思？）&lt;/li&gt;
  &lt;li&gt;共指解析（例如，“他”和“它”在文档中分别指代什么？）&lt;/li&gt;
  &lt;li&gt;问答系统&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;2-词的表示&quot;&gt;2 词的表示&lt;/h1&gt;

&lt;h2 id=&quot;21-如何表示词&quot;&gt;2.1 如何表示词&lt;/h2&gt;

&lt;p&gt;那么我们该如何使计算机明白一个词、一句话，乃至一篇文章的意思呢？&lt;/p&gt;

&lt;p&gt;我们先举个实际例子，大家或许都知道 WorldNet ，他是自然语言处理工具包 NLTK 包含一个同义词集（synonym sets）和上位词（hypernyms）的词库。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;hypernyms :&lt;/strong&gt;  “is a” 的关系。如，Rose is a flower。Flower is a plant。即 “花” 是 “玫瑰” 的上位词，“植物” 是 “花” 的上位词。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cs224n/worldnet_demo.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到它是一个可以很好地进行同义词比对和上位词查找的词库。那么，像这种基于人工统计的词库有没有缺点呢？答案是肯定的。&lt;/p&gt;

&lt;p&gt;首先，它不能判别词的细微差别，如，“proficient” 是 “good” 的同义词，但是在一些文章中并不是。第二，它缺少了很多新词，如，badass, nifty, wizard, ninja 等，再进一步，通过 WorldNet 的组织形式可以看出，要加入新词几乎是不可操作的。第三，由于是人工统计的，显得太主观。第四，需要大量的人力和物力去创建和维护。还有，它不能定量地计算词与词之间的相似度。&lt;/p&gt;

&lt;p&gt;为了让大多数的自然语言处理任务能有更好的表现，我们先需要了解单词之间的相似和不同，于是词向量应运而生。&lt;/p&gt;

&lt;h2 id=&quot;22-词向量&quot;&gt;2.2 词向量&lt;/h2&gt;

&lt;p&gt;在所有的 NLP 任务中，最重要的是我们如何将单词表示为任意模型的输入。而模型的输入是一组数字，于是很自然地想到用向量来表示一个词，即把单词映射为实数向量，这种向量就叫做词向量。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;词向量（word vector）&lt;/strong&gt; 有时又被称作&lt;strong&gt;词嵌入 （word embedding）&lt;/strong&gt; 或者&lt;strong&gt;词表示（word representation）&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;一共有1千3百万个左右的英语单词，它们其中很多都是有关系的。例如 “feline” 和 “cat”，“hotel” 和 “motel” 。因此，我们希望用词向量编码单词使它嵌入到词组空间中，代表其中的一个点（这也是词向量称为词嵌入的原因）。这样做最直观的原因是，在实际中可能存在 $N$ 维空间（$N$ $\ll$ $13 million$）足以编码所有单词以及语义。每个维度都会编码一些使用言语传达的意思。例如，语义维度可能表示时态（过去、现在和未来），计数（单数和复数）和性别（男性和女性）。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;这里先提前区别几组名词：&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;distributed representation&lt;/strong&gt; (密集型表示) 与 &lt;strong&gt;symbolic representation&lt;/strong&gt;（localist representation、one-hot representation）相对。&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;discrete representation&lt;/strong&gt; (离散表示) 与 &lt;strong&gt;symbolic representation&lt;/strong&gt; (符号表示) 及 &lt;strong&gt;denotation&lt;/strong&gt; 的意思相似。&lt;/p&gt;

  &lt;p&gt;切不可搞混 &lt;strong&gt;distributed&lt;/strong&gt; 和 &lt;strong&gt;discrete&lt;/strong&gt; 这两个词。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我们从最简单的开始，传统的 NLP 的做法是将词看作是一组离散的符号，将词表示成 &lt;strong&gt;one-hot 向量&lt;/strong&gt;：每个词都是一个 $\mathbb{R}^{\mid V\mid \times 1}$ 向量，其中除了该单词所在的索引为 1 外其他索引都是 0。在这个定义下， $\mid V\mid$ 是词汇表的大小，即词表中词的个数。这时词向量的可以表示为&lt;/p&gt;

&lt;p&gt;$hotel$  = [ 0 0 0 0 1 0 0 ··· 0 ]&lt;/p&gt;

&lt;p&gt;$motel$ = [ 0 1 0 0 0 0 0 ··· 0 ]&lt;/p&gt;

&lt;p&gt;$cat$    = [ 0 0 0 1 0 0 0 ··· 0 ]&lt;/p&gt;

&lt;p&gt;但是，这样的表示无法给出词之间的相似性，因为根据相似度的一种定义（内积）：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left(w^{\text { hotel }}\right)^{T} w^{\text { motel }}=\left(w^{\text { hotel }}\right)^{T} w^{\text { cat }}=0&lt;/script&gt;

&lt;p&gt;很不可思议吧，$hotel$ 与 $motel$ 的相似度竟然等于 $hotel$ 和 $cat$ 的相似度。这是因为维数越大，数据越稀疏，比较相似度往往缺乏实际意义。&lt;/p&gt;

&lt;p&gt;那么接下来很明确，就是降低维度，使数据变得稠密，找到一个更低维度的向量空间来编码词与词之间的关系。&lt;/p&gt;

&lt;h2 id=&quot;23-共现矩阵&quot;&gt;2.3 共现矩阵&lt;/h2&gt;

&lt;p&gt;在介绍降维的方法之前，我先引入共现矩阵（co-occurrence）的概念。共现矩阵也叫共现计数矩阵，是基于统计方法得到的矩阵，记作 $X$。&lt;/p&gt;

&lt;h3 id=&quot;231-基于文档的共现矩阵&quot;&gt;2.3.1 基于文档的共现矩阵&lt;/h3&gt;

&lt;p&gt;我们猜想，有关联的词经常会出现在同一个文档中，例如，“banks”，“stocks”，“shares” 等，出现在同一篇的文档的概率较高，而 “banks”，“banana”，“phone”出现在同一篇文档的概率较小。根据这种情况，我们可以建立一个词-文档矩阵 $X$ ，$X$ 按照以下方式构建：遍历文档，当词 $i$ 出现在文档 $j$ ，我们对 $X_{ij}$ 加一，遍历结束，我们便可得到一个词-文档矩阵。但这显然是一个很大的矩阵$X\in\mathbb{R}^{\mid V\mid \times M}$ ，它的规模是和文档数 $M$ 成正比的，而 $M$ 通常非常大（$billion$级），因此，不考虑这种方式。&lt;/p&gt;

&lt;h3 id=&quot;232-基于窗口的共现矩阵&quot;&gt;2.3.2 基于窗口的共现矩阵&lt;/h3&gt;

&lt;p&gt;使用基于窗口的方式，我们直接通过单词与单词之间的共现来计算共现矩阵。在这种方法中，我们统计每个单词在感兴趣单词的附近特定大小的窗口中出现的次数。我们按照这个方法对语料库中的所有单词进行统计。当窗口大小为2时，窗口是这样移动的，直至最后一个单词。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cs224n/window_based.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;举个例子，语料库由三句话组成，窗口的大小是 1（考虑中心词左右两侧的第一个单词）：&lt;/p&gt;

&lt;p&gt;I enjoy flying.&lt;/p&gt;

&lt;p&gt;I like NLP.&lt;/p&gt;

&lt;p&gt;I like deep learning.&lt;/p&gt;

&lt;p&gt;共现矩阵如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cs224n/co_occurrence_matrix.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;24-svd&quot;&gt;2.4 SVD&lt;/h2&gt;

&lt;p&gt;这里先提两个概念：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Denotational semantics:&lt;/strong&gt; The concept of representing an idea as a symbol (a word or a one-hot vector). It is sparse and cannot capture similarity. This is a “localist” representation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Distributional semantics:&lt;/strong&gt; The concept of representing the meaning of a word based on the context in which it usually appears. It is dense and can better capture similarity.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;简而言之，就是 &lt;strong&gt;Denotational semantics&lt;/strong&gt; 是用符号表示词，稀疏，无法获得相似性，而 &lt;strong&gt;Distributional semantics&lt;/strong&gt; 是用上下文表示词，稠密，可以很好地获取相似性。我们在上面说的 one-hot 向量表示词就是一种 &lt;strong&gt;Denotational semantics&lt;/strong&gt;，而以下要说的 SVD 方法就是一种  &lt;strong&gt;Distributional semantics&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;SVD即奇异值分解，是矩阵分解的一种。SVD 方法是一种找到词向量的方法，首先遍历一个很大的数据集和统计词的共现计数矩阵 $X$，然后对矩阵 $X$ 进行 SVD 分解得到 $USV^{T}$ （$U,S,V\in\mathbb{R}^{\mid V\mid \times \mid V\mid}$）。通过选择前 $k$ 个奇异值来降低维度，然后使用 $U_{1:\mid V\mid,1:k} \in \mathbb{R}^{\mid V\mid \times k}$ 的行向量作为词汇表中所有词的词向量，词向量的维度为 $k$。&lt;/p&gt;

&lt;p&gt;SVD 方法能让我们的词向量编码充分的语义和句法（词性标注）的信息，但是也会存在许多问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;共现矩阵的维度会经常发生改变（经常增加新的单词和语料库的大小会改变）。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;共现矩阵会非常的稀疏，因为很多词不会共现。&lt;/li&gt;
  &lt;li&gt;共现矩阵的维度一般会非常高（$ \approx 10^{6} \times 10^{6}$ ）。&lt;/li&gt;
  &lt;li&gt;基于 SVD 的方法的计算复杂度一般为 $O\left(mn^{2}\right)$ 。&lt;/li&gt;
  &lt;li&gt;需要在 $X$ 上加入一些技巧处理来解决词频的不平衡。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对上述讨论中存在的问题有以下的解决方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;忽略功能词，例如 “the”，“he”，“has” 等等。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;使用 ramp window ，即根据中心词与上下文词之间的距离远近，赋予共现计数不同的权重。&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;如，和中心词最相邻的上下文词的计数权重为1，相隔5个位置的计数权重为0.5。&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;使用皮尔逊相关系数将负数的计数设为 0，而不是使用原始的计数。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在下一部分，基于迭代的方法可以用一种更为优雅的方式解决大部分上述问题。&lt;/p&gt;

&lt;h1 id=&quot;3-word2vec&quot;&gt;3 word2vec&lt;/h1&gt;

&lt;p&gt;我们回顾一下文章开头的话：You shall know a word by the company it keeps。这是 J. R. Firth 大佬说的，他是现代统计自然语言处理最成功的思想之一。简单来说，就是我们要到具体的上下文当中去理解一个词。上一节中提出的共现矩阵其实就是一种捕捉单词上下文词信息的方法。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cs224n/context_word.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是一种从大量文本语料中学习语义知识的模型，它被大量地用在自然语言处理（NLP）中。它的模型参数就是词向量，用词向量表征词的语义信息，词向量可作为下游任务的输入。&lt;/p&gt;

&lt;p&gt;word2vec 模型实际上分为了两个部分，第一部分为建立模型，第二部分是通过模型获取嵌入词向量。word2vec的整个建模过程实际上与自编码器（auto-encoder）的思想很相似，即先基于训练数据构建一个神经网络，当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵——后面我们将会看到这些权重在 word2vec 中实际上就是我们试图去学习的“词向量”。基于训练数据建模的过程，我们给它一个名字叫“Fake Task”，意味着建模并不是我们最终的目的。&lt;/p&gt;

&lt;p&gt;word2vec 模型主要有 Skip-Gram 和 Continuous Bag-of-Word 两种模型，从直观上理解，Skip-Gram 是给定中心词（center word）来预测上下文词（context word），而 CBOW 是给定上下文词（context word），来预测中心词（center word）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cs224n/SG_arc.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;center&gt;Skip-Gram 模型&lt;/center&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cs224n/CBOW_arc.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;center&gt;CBOW 模型&lt;/center&gt;

&lt;h2 id=&quot;31-language-model&quot;&gt;3.1 Language Model&lt;/h2&gt;

&lt;p&gt;在正式介绍 word2vec 模型之前，我们需要了解一些语言模型（language model）相关的概念，我们从一个简单的例子开始：&lt;/p&gt;

&lt;p&gt;“ The cat jumped over the puddle. ”​&lt;/p&gt;

&lt;p&gt;一个好的语言模型会给这个句子很高的概率，因为在句法和语义上这是一个完全有效的句子。相似地，句子 “ stock boil fish is toy. ” 会得到一个很低的概率，因为这是一个无意义的句子。在数学上，我们可以称给定 n 个词的序列的概率是：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P\left(w_{1}, w_{2}, \cdots, w_{n}\right)&lt;/script&gt;

&lt;p&gt;我们可以使用 unigram 语言模型方法，通过假设单词的出现是符合独立同分布假设的：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P\left(w_{1}, w_{2}, \cdots, w_{n}\right)=\prod_{i=1}^n P\left(w_{i}\right)&lt;/script&gt;

&lt;p&gt;但是我们知道这是不合理的，因为下一个单词是高度依赖于前面的单词序列的。如果使用上述的语言模型，可能会让一个无意义的句子具有很高的概率（只需要选取那些出现频率高的单词组成一个句子）。所以我们可以让序列的概率 等于 序列中每个单词和其旁边的单词组成单词对的概率的乘积。我们称之为 bigram 模型：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P\left(w_{1}, w_{2}, \cdots, w_{n}\right)=\prod_{i=2}^{n} P\left(w_{i} | w_{i-1}\right)&lt;/script&gt;

&lt;p&gt;虽然这个方法会有提升，但还是过于简单，不能捕获完整的语义信息，因为我们只关心一对邻近的单词，而不是针对整个句子来考虑。考虑在词-词共现矩阵中，共现窗口为 1，我们基本上能得到这样的成对的概率。但是，这需要计算和存储大量数据集的全局信息。&lt;/p&gt;

&lt;p&gt;类似的方法还有 trigram 模型、four-gram 模型等，统称为 n-gram 模型。这些模型的计算非常复杂，在实际情况中最多取 four-gram，原则上能用 trigram 解决的问题绝对不用 four-gram。&lt;/p&gt;

&lt;p&gt;现在我们知道该如何计算一个序列的概率，接下来就是 word2vec 中一些可以计算这些概率的模型。使用迭代的方法简化模型使得训练模型速度非常快，而不是像 n-gram 模型一样去计算和维护一个庞大数据集得到全局信息。&lt;/p&gt;

&lt;h2 id=&quot;32-the-fake-task&quot;&gt;3.2 The Fake Task&lt;/h2&gt;

&lt;h3 id=&quot;321-任务介绍&quot;&gt;3.2.1 任务介绍&lt;/h3&gt;

&lt;p&gt;我们在介绍 word2vec 的时候提到，训练模型的真正目的是获得模型基于训练数据学得的隐层权重。为了得到这些权重，我们首先要构建一个完整的神经网络作为我们的“Fake Task”，后面再返回来通过“Fake Task”间接地得到这些词向量。&lt;/p&gt;

&lt;p&gt;接下来我们来看看如何训练我们的神经网络。假如我们有一个句子： $\text {“The dog barked at the mailman”}$。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;首先我们选句子中间的一个词作为我们的输入词，例如我们选取“dog”作为中心词；&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;这里做一个规定：对于 CBOW 模型，input word 是上下文词，output word 是中心词，而对于 Skip-Gram 模型，input word 是中心词，output word 是上下文词。标记有点多，千万不要混淆。&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;我们再定义一个 window_size 参数，它代表着我们从当前中心词的一侧（左边或右边）选取词的数量。如果我们设置 window_size = 2 ，那么我们最终获得窗口中的词（包括中心词在内）就是[‘The’, ‘dog’，’barked’, ‘at’]。window_size = 2 代表着选取中心词左侧2个词和右侧2个词进入我们的窗口，所以整个窗口大小 span = 4。另一个参数叫 num_skips，它代表着我们从整个窗口中选取多少个&lt;strong&gt;不同的词&lt;/strong&gt;作为我们的 output word，当 window_size = 2，num_skips = 2 时，我们将会得到两组 (input word, output word) 形式的训练数据，即 (‘dog’, ‘barked’)，(‘dog’, ‘the’)。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;神经网络基于这些训练数据将会输出一个概率分布，这个概率代表在给定 input word 的情况下词表中的每个词出现的概率。再用交叉熵损失将这个概率分布与 output word 的 one-hot 向量的误差反向传播。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;模型的输出概率代表词表中每个词有多大可能性跟 input word 同时出现。举个栗子，如果我们向神经网络模型中输入一个单词“Soviet”，那么最终模型的输出概率中，像“Union”， “Russia”这种相关词的概率将远高于像“watermelon”，“kangaroo”非相关词的概率。因为“Union”，“Russia”更有可能在“Soviet”的窗口中出现。
我们将通过给神经网络输入文本中成对的单词来训练它，完成上面所说的概率计算。下面的图中给出了一些我们的训练样本的例子。&lt;/p&gt;

&lt;p&gt;假设有句子：$\text{ “The quick brown fox jumps over lazy dog. ”}$ ，设定我们的窗口大小为2（window_size = 2）。下图中，蓝色代表input word，方框内代表位于窗口内的单词。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cs224n/training_data.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;模型将会从每对单词出现的次数中学习得到统计结果。例如，我们的神经网络可能会得到更多类似（“Soviet”，“Union”）这样的训练样本对，而（“Soviet”，“Sasquatch”）这样的训练样本对却很少。因此，当我们的模型完成训练后，给定一个单词“Soviet”作为输入，输出的结果中“Union”或者“Russia”要比“Sasquatch”的概率大得多。&lt;/p&gt;

&lt;h3 id=&quot;322--任务实施&quot;&gt;3.2.2  任务实施&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;1  输入层&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;我们首先从最简单情形开始，input word 只有一个，即给定一个 input word，预测 output word（有点像 bigram 模型）。假设词表大小为$V$，隐藏层神经元个数为$N$，输入层到隐藏层、隐藏层到输出层都是全连接，样本是（input word，output word）单词对，输入为 input word 的 one-hot 向量。&lt;/p&gt;

&lt;p&gt;假设从我们的训练文档中抽取出 10000 个不重复的单词组成词汇表。我们对这 10000 个单词进行 one-hot 编码，得到的每个单词都是一个 10000 维的向量，向量每个维度的值只有 0 或者 1，假如单词“ants”在词汇表中的出现位置为第 3 个，那么“ants”的向量就是 $\text {ants}=[0,0,1,0, \ldots, 0]$ 。&lt;/p&gt;

&lt;p&gt;模型的输入如果为一个 10000 维的向量，那么输出也是一个 10000 维度（词汇表的大小）的向量，它包含了 10000 个概率，每一个概率代表着给定 input word 词表中的每个词出现在 input word 附近的概率大小。&lt;/p&gt;

&lt;p&gt;下图是我们神经网络的结构：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cs224n/one_word_net_arch.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;隐层没有使用任何激活函数，但是输出层使用了sotfmax。&lt;/p&gt;

&lt;p&gt;我们使用成对的单词来对神经网络进行训练，训练样本是 ( input word, output word ) 这样的单词对，input word 和 output word 都是 one-hot 编码的向量。最终模型的输出是一个概率分布，每一个输出神经元都是一个概率值，表示在给定 input word 的情况下词表中每个词出现的概率，即 $p\left(w_{j} \mid w_{I}\right)$，$w_{I}$ 表示 input word，$w_{j}$，表示词表中的词，$\text j=1,2, \ldots, V$ 。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2  隐藏层&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;说完单词的编码和训练样本的选取，我们来看下我们的隐层。如果我们现在想用300个特征来表示一个单词（即每个词可以被表示为300维的向量）。那么隐层的权重矩阵应该为10000行（词汇表中的每个单词），300列（每个隐藏神经元）。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Google在最新发布的基于Google news数据集训练的模型中使用的就是300维的词向量。词向量的维度是一个可以调节的超参数，可以根据不同的任务调节，实践中一般300维为佳。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;看下面的图片，左右两张图分别从不同角度代表了输入层-隐层的权重矩阵。左图中每一列代表一个10000维的词向量和隐层单个神经元连接的权重向量。从右边的图来看，每一行实际上代表了每个单词的词向量。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cs224n/word2vec_weight_matrix_lookup_table.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;现在我们已经完成了“Fake Taks”，我们最终的目标就是学习到隐层的权重矩阵。&lt;/p&gt;

&lt;p&gt;我们现在回来，接着通过模型的定义来训练我们的这个模型。&lt;/p&gt;

&lt;p&gt;上面我们提到，input word 和 output word 都会被我们进行 one-hot 编码。仔细想一下，我们的输入被 one-hot 编码以后大多数维度上都是0（实际上仅有一个位置为1），所以这个向量相当稀疏，那么会造成什么结果呢。如果我们应用&lt;strong&gt;矩阵乘法&lt;/strong&gt;将一个1 x 10000的向量和10000 x 300的矩阵相乘，它会消耗相当大的计算资源，为了高效计算，它仅仅会选择矩阵中对应的向量中维度值为1的索引行，看图就明白。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cs224n/matrix_mult_w_one_hot.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;为了有效地进行计算，这种稀疏状态下不会进行矩阵乘法计算，可以看到计算的结果实际上是矩阵的某一行：先根据 input word 的 one-hot 向量中元素 1 的索引，再由这个索引取得矩阵对应的行。上面的例子中，左边向量中取值为 1 的对应维度为 3（下标从0开始），那么计算结果就是矩阵的第 3 行（下标从0开始）—— [10  12  19]，这样模型中的隐层权重矩阵便成了一个“查找表”（lookup table），进行矩阵计算时，直接去查输入向量中取值为1的维度下对应的那些权重值。隐层的输出就是每个输入单词的“嵌入词向量”，也称为输入词向量。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;上面提到的隐层权重矩阵是从输入层到隐藏层的权重矩阵，称为&lt;strong&gt;输入词向量矩阵&lt;/strong&gt;，在接下来的章节中我们还会看到，从隐藏层到输出层的权重矩阵，称为&lt;strong&gt;输出词向量矩阵&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;3  输出层&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;经过神经网络隐层的计算，“ants“ 这个词会从一个 1 x 10000 的向量变成 1 x 300 的向量，再被输入到输出层。输出层是一个softmax回归分类器，它的每个结点将会输出一个 0-1 之间的值（概率），这些所有输出层神经元结点的概率之和为1。&lt;/p&gt;

&lt;p&gt;下面是一个例子，训练样本为 (input word: ants， output word: car) 的计算示意图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cs224n/output_weights_function.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;乘号右侧的 output weights for “car” 即是 “car” 的输出词向量，“car”在作为 input word 时也有相应的输入词向量。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;直觉上的理解，如果两个不同的单词有着非常相似的“上下文”（也就是窗口单词很相似，比如“Kitty climbed the tree”和“Cat climbed the tree”），那么通过我们的模型训练，这两个单词的嵌入向量将非常相似。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;在word2vec 中，规定两个单词的&lt;strong&gt;相似度&lt;/strong&gt;就是两者词向量的内积，如，上面的例子“ants”和“car”的计算结果就是两者的相似度。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;那么两个单词拥有相似的“上下文”到底是什么含义呢？比如对于同义词“intelligent”和“smart”，我们觉得这两个单词应该拥有相同的“上下文”。而例如“engine”和“transmission”这样相关的词语，可能也拥有着相似的上下文。&lt;/p&gt;

&lt;p&gt;实际上，这种方法实际上也可以帮助你进行词干化（stemming），例如，神经网络对“ant”和“ants”两个单词会习得相似的词向量。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;词干化（stemming）就是去除词缀得到词根的过程。如，looks/looked/looking $\to$ look，happier/happiest $\to$ happy 等。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;以上便是 word2vec 模型的最简化形式，CBOW 和 Skip-Gram 无非就是对这个模型的推广，到这里，大家应该在直观上理解了这个模型，如若要深究下去（数学推导👀），我会在下一篇文章中具体分析。&lt;/p&gt;

&lt;h1 id=&quot;4-小结&quot;&gt;4 小结&lt;/h1&gt;

&lt;p&gt;这篇文章从自然语言开始讲起，通过如何表示一个词，讲述了从词的离散表示到稠密表示的发展过程，引入了共现矩阵、词向量、SVD、word2vec 等方法将一个具体的单词（符号）表示成可以喂给任意模型处理的数据。在下一篇文章中，我将介绍 word2vec 的两种具体的模型：CBOW 和 Skip-Gram，以及它们各自的目标函数梯度的计算和梯度下降算法，还有它们的训练方法：负采样 (Negative sampling)和 Hierarchical softmax。&lt;/p&gt;

&lt;h2 id=&quot;参考文献&quot;&gt;参考文献&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/&quot;&gt;Word2Vec Tutorial - The Skip-Gram&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://web.stanford.edu/class/cs224n/&quot;&gt;CS224n notes-01&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Fri, 16 Aug 2019 04:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/08/16/Stanford-CS224n-nlp-02/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/08/16/Stanford-CS224n-nlp-02/</guid>
        
        <category>NLP</category>
        
        <category>Stanford</category>
        
        <category>深度学习</category>
        
        
      </item>
    
      <item>
        <title>Stanford CS224n 自然语言处理（一）</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;Begining&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;

&lt;p&gt;我是在4月初入坑自然语言处理的，并在4月初到6月初的2个月时间里面学习了斯坦福的自然语言处理课程。接下来我会大致介绍一下这门课程，讲讲每节课的大纲，以及我觉得缺少的东西。&lt;/p&gt;

&lt;p&gt;首先给出课程的&lt;a href=&quot;http://web.stanford.edu/class/cs224n/&quot;&gt;官方网站&lt;/a&gt;，所有的课程资料都可以在这个网页上下载，包括：课程ppt、作业说明文档、手册、部分笔记等。配套的网络课程在&lt;a href=&quot;https://www.bilibili.com/video/av46065585?from=search&amp;amp;seid=12274272452479951197&quot;&gt;这里&lt;/a&gt;，如果你希望系统地学习 NLP，那么这门课程是你的首选。(6月下旬斯坦福又开放了一门课程 CS224u 自然语言理解，我会另开一个系列介绍)&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;课程大纲&quot;&gt;课程大纲&lt;/h2&gt;

&lt;h4 id=&quot;lecture-1--introduction-and-word-vectors&quot;&gt;Lecture 1 : Introduction and Word Vectors&lt;/h4&gt;

&lt;p&gt;首先介绍了课程的教授者，课程资料以及作业的评分细则，然后列出了和2017年的 CS2224n 课程的不同点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;课程涵盖新的模型和方法，如，字符级模型( Character Model )、Transformer、多任务学习( Multitask Learning )&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;作业涵盖了新内容，如，NMT with attention、卷积网络( ConvNets )、子词模型( subword model )&lt;/li&gt;
  &lt;li&gt;使用 PyTorch 而不是 TensorFlow&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;接着介绍了自然语言以及词义；词向量、word2vec 概览；word2vec 的目标函数梯度的计算以及梯度下降算法。&lt;/p&gt;

&lt;h4 id=&quot;lecture-2--word-vectors-and-word-senses&quot;&gt;Lecture 2 : Word Vectors and Word Senses&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;实现 word2vec 的两种模型
    &lt;ul&gt;
      &lt;li&gt;Skip-grams (SG)&lt;/li&gt;
      &lt;li&gt;Continuous Bag of Words (CBOW)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;训练技巧
    &lt;ul&gt;
      &lt;li&gt;负采样 (Negative sampling)&lt;/li&gt;
      &lt;li&gt;Hierarchical softmax&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;共现矩阵( co-occurrence matrix )，以及降维方法( SVD )&lt;/li&gt;
  &lt;li&gt;GloVe&lt;/li&gt;
  &lt;li&gt;词向量的评价指标&lt;/li&gt;
  &lt;li&gt;词义：歧义( word sense ambiguity )、一词多义( polysemy )等&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-3--word-window-classification-neural-networks-and-matrix-calculus&quot;&gt;Lecture 3 : Word Window Classification, Neural Networks and Matrix Calculus&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;分类问题概览&lt;/li&gt;
  &lt;li&gt;神经网络简介&lt;/li&gt;
  &lt;li&gt;命名实体识别( Named Entity Recognition )&lt;/li&gt;
  &lt;li&gt;词窗口&lt;/li&gt;
  &lt;li&gt;矩阵求导&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-4--backpropagation-and-computation-graphs&quot;&gt;Lecture 4 : Backpropagation and computation graphs&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;推导矩阵反向传播的梯度&lt;/li&gt;
  &lt;li&gt;计算图( computation graphs )&lt;/li&gt;
  &lt;li&gt;神经网络的一些需要知道的问题：
    &lt;ul&gt;
      &lt;li&gt;向量化表达( Vectorization ) / 矩阵化表达( Matrixization )的计算速度优势&lt;/li&gt;
      &lt;li&gt;过拟合( overfitting )&lt;/li&gt;
      &lt;li&gt;非线性( nonlinearities )&lt;/li&gt;
      &lt;li&gt;初始化( initialization )&lt;/li&gt;
      &lt;li&gt;优化器( optimizers )&lt;/li&gt;
      &lt;li&gt;学习率( learning rates )&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-5--dependency-parsing&quot;&gt;Lecture 5 : Dependency Parsing&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;两种句法结构：
    &lt;ul&gt;
      &lt;li&gt;一致性( Consistency ) ，即短语结构语法，也即无上下文语法( context-free grammars )，18课会单独讲&lt;/li&gt;
      &lt;li&gt;依存性( Dependency )，即依存结构&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;依存语法和依存结构&lt;/li&gt;
  &lt;li&gt;依存关系解析( Dependency Parsing )
    &lt;ul&gt;
      &lt;li&gt;基于转换的解析( transition-based )&lt;/li&gt;
      &lt;li&gt;基于神经网络的解析( nueral )&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-6--language-models-and-recurrent-neural-networks&quot;&gt;Lecture 6 : Language Models and Recurrent Neural Networks&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;语言模型简介&lt;/li&gt;
  &lt;li&gt;n-gram 语言模型&lt;/li&gt;
  &lt;li&gt;神经网络（RNN）语言模型&lt;/li&gt;
  &lt;li&gt;评估语言模型&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-7--vanishing-gradients-and-fancy-rnns&quot;&gt;Lecture 7 : Vanishing Gradients and Fancy RNNs&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;梯度消失问题和梯度爆炸问题及其解决方法&lt;/li&gt;
  &lt;li&gt;LSTM （ Long Short-Term Memory ）&lt;/li&gt;
  &lt;li&gt;GRU（ Gated Recurrent Unit ）&lt;/li&gt;
  &lt;li&gt;RNN variant：
    &lt;ul&gt;
      &lt;li&gt;Bidirectional RNN&lt;/li&gt;
      &lt;li&gt;Multi-layer RNN&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-8--machine-translation-sequence-to-sequence-and-attention&quot;&gt;Lecture 8 : Machine Translation, Sequence-to-sequence and Attention&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;统计机器翻译（ SMT ）
    &lt;ul&gt;
      &lt;li&gt;对齐（alignment）&lt;/li&gt;
      &lt;li&gt;解码（decodeing）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;神经机器翻译（ NMT ）
    &lt;ul&gt;
      &lt;li&gt;seq2seq&lt;/li&gt;
      &lt;li&gt;beam search decoding&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Attention 机制&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-9--practical-tips-for-final-projects&quot;&gt;Lecture 9 : Practical Tips for Final Projects&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;最终项目的一些建议&lt;/li&gt;
  &lt;li&gt;回顾 GRU、LSTM&lt;/li&gt;
  &lt;li&gt;机器翻译的评价指标：BLEU&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-10--textual-question-answering&quot;&gt;Lecture 10 : (Textual) Question Answering&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;QA —&amp;gt;机器阅读理解&lt;/li&gt;
  &lt;li&gt;SQuAD 数据集&lt;/li&gt;
  &lt;li&gt;斯坦福 Attentive Reader&lt;/li&gt;
  &lt;li&gt;BiDAF&lt;/li&gt;
  &lt;li&gt;一些先进的模型：Co-attention、FusionNet 等&lt;/li&gt;
  &lt;li&gt;ELMo 和 BERT 概览&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-11--convnets-for-nlp&quot;&gt;Lecture 11 : ConvNets for NLP&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;从 RNN 到 CNN&lt;/li&gt;
  &lt;li&gt;模型比较&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-12--information-from-parts-of-words--subword-models&quot;&gt;Lecture 12 : Information from parts of words : Subword Models&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;语言学概览
    &lt;ul&gt;
      &lt;li&gt;发音（Phonetics）&lt;/li&gt;
      &lt;li&gt;音韵（Phonology）&lt;/li&gt;
      &lt;li&gt;词的形态（Morphology）&lt;/li&gt;
      &lt;li&gt;书写系统&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;字符级的模型&lt;/li&gt;
  &lt;li&gt;子词模型
    &lt;ul&gt;
      &lt;li&gt;Byte Pair Encoding&lt;/li&gt;
      &lt;li&gt;Wordpiece&lt;/li&gt;
      &lt;li&gt;Sentencepiece&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;混合神经网络翻译&lt;/li&gt;
  &lt;li&gt;fastText&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-13--contextual-word-representations-and-pretraining&quot;&gt;Lecture 13 : Contextual Word Representations and Pretraining&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;词表示&lt;/li&gt;
  &lt;li&gt;pre-ELMo &amp;amp; EMLo&lt;/li&gt;
  &lt;li&gt;ULMfit&lt;/li&gt;
  &lt;li&gt;Transformer 架构
    &lt;ul&gt;
      &lt;li&gt;self-attention&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;BERT&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-14--self-attention-for-generative-models&quot;&gt;Lecture 14 : Self-Attention for Generative Models&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;Self-Attention&lt;/li&gt;
  &lt;li&gt;文本生成&lt;/li&gt;
  &lt;li&gt;图像生成&lt;/li&gt;
  &lt;li&gt;音乐生成&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-15--natural-language-generation&quot;&gt;Lecture 15 : Natural Language Generation&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;回顾语言模型和解码算法及技巧
    &lt;ul&gt;
      &lt;li&gt;Greedy decoding&lt;/li&gt;
      &lt;li&gt;Beam search&lt;/li&gt;
      &lt;li&gt;Sampling methods&lt;/li&gt;
      &lt;li&gt;Softmax temperature&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;自然语言生成（NLG）任务以及神经网络方法
    &lt;ul&gt;
      &lt;li&gt;文本总结&lt;/li&gt;
      &lt;li&gt;对话系统&lt;/li&gt;
      &lt;li&gt;创意写作&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;NLG 的评估&lt;/li&gt;
  &lt;li&gt;NLG 的趋势和发展&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-16--coreference-resolution&quot;&gt;Lecture 16 : Coreference Resolution&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;共指解析及其应用&lt;/li&gt;
  &lt;li&gt;提及检测（Mention Detection）&lt;/li&gt;
  &lt;li&gt;一些语言学概念
    &lt;ul&gt;
      &lt;li&gt;anaphora&lt;/li&gt;
      &lt;li&gt;cataphora&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;共指模型
    &lt;ul&gt;
      &lt;li&gt;Rule-based Model&lt;/li&gt;
      &lt;li&gt;Mention Pair Model&lt;/li&gt;
      &lt;li&gt;Mention Ranking Model&lt;/li&gt;
      &lt;li&gt;Mention Clustering Model&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;评估指标&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-17--multitask-learning-as-qa&quot;&gt;Lecture 17 : Multitask Learning as QA&lt;/h4&gt;

&lt;p&gt;这节课相当于一个讲座，介绍了多任务学习的若干模型、训练策略，以及多任务学习的优势&lt;/p&gt;

&lt;h4 id=&quot;lecture-18--tree-recursive-neural-networks-constituency-parsing-and-sentiment&quot;&gt;Lecture 18 : Tree Recursive Neural Networks, Constituency Parsing, and Sentiment&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;递归神经网络（Recursive Neural Networks）&lt;/li&gt;
  &lt;li&gt;短语句法解析&lt;/li&gt;
  &lt;li&gt;树递归神经网络（TreeRNN）&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-19--bias-in-the-vision-and-language-of-artificial-intelligence&quot;&gt;Lecture 19 : Bias in the Vision and Language of Artificial Intelligence&lt;/h4&gt;

&lt;p&gt;讲座，介绍了 AI 的偏见以及多任务学习&lt;/p&gt;

&lt;h4 id=&quot;lecture-20--the-future-of-deep-learning--nlp&quot;&gt;Lecture 20 : The Future of Deep Learning + NLP&lt;/h4&gt;

&lt;p&gt;讲座，介绍了大规模深度学习在各个领域取得的成就，无标签数据集，无监督机器翻译，大模型（如 BERT、GPT-2），多任务学习，更加难的自然语言理解，NLP 在工业上的应用，以及未来的挑战。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;后记&quot;&gt;后记&lt;/h2&gt;

&lt;p&gt;以上就是Stanford CS224n 自然语言处理课程的课程概览，我会在今后陆续完成每节课的分析与笔记（如果有时间的话 hhh）。&lt;/p&gt;

&lt;p&gt;自然语言处理真的是一门非常庞大的学科，其中包含了很多内容，想要在一门课上学习完所有的内容是不现实的，个人所见，这门课的的意义就在于为刚开始学习自然语言处理的学生做一个相对详细的 review ，接下去要做什么方向，按照自己的兴趣来。&lt;/p&gt;

&lt;p&gt;在前言中我提到过，这门课程还有许多没有提及的方面，比如分词方法、TF-IDF、关系提取、情感分析等，如果对这些感兴趣的话，Stanford 为大家又提供了一门课程（&lt;a href=&quot;http://web.stanford.edu/class/cs224u/&quot;&gt;CS224u 自然语言理解&lt;/a&gt;），感谢斯坦福。&lt;/p&gt;

&lt;p&gt;写这些文章的初衷是为了对抗遗忘，所以某种程度上是给自己温故用的。因此，我不会把课上的内容完完整整地记录下来，如果有什么疑问，欢迎指出。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PS&lt;/strong&gt; ：博客的评论区还没有调试成功，如果我的文章有什么错误，烦请点击下面的微博按钮，私信给我，不胜感激。&lt;/p&gt;

&lt;hr /&gt;

</description>
        <pubDate>Sun, 04 Aug 2019 23:04:00 +0800</pubDate>
        <link>http://localhost:4000/2019/08/04/Stanford-CS224n-nlp-01/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/08/04/Stanford-CS224n-nlp-01/</guid>
        
        <category>NLP</category>
        
        <category>Stanford</category>
        
        <category>深度学习</category>
        
        
      </item>
    
      <item>
        <title>Hello World</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;“Yeah It’s on.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;

&lt;p&gt;我的Blog就这么开通了。&lt;/p&gt;

&lt;p&gt;鼓捣了两天，不容易啊。。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#build&quot;&gt;这里推荐几个写的比较好的教程&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;其实很早就想建一个小站，记录一下学习和生活，因为自己记性不好，总想着留下点印记，以便回溯，那么就从今日开始吧。&lt;/p&gt;

&lt;p id=&quot;build&quot;&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;a href=&quot;http://bluebiu.com/blog/create-blog-in-github.html&quot;&gt;从零开始用GitHub Pages创建博客&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;[http://bluebiu.com/blog/learn-to-use-jekyll.html#12%E7%AE%80%E5%8D%95%E5%A5%97%E7%94%A8](http://bluebiu.com/blog/learn-to-use-jekyll.html#12简单套用)&quot;&gt;从零开始折腾Jekyll&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://ambeta.github.io/2016/03/27/build-website-with-jekyll.html#jekyll-template&quot;&gt;如何使用jekyll建站&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://chaosinmotion.coding.me/cblog/2016/03/26/build-a-blog/&quot;&gt;Github Pages + Jekyll 独立博客一小时快速搭建&amp;amp;上线指南&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;正文&quot;&gt;正文&lt;/h2&gt;

&lt;p&gt;接下来说说搭建这个博客的过程。&lt;/p&gt;

&lt;p&gt;我的操作系统是macOS，一开始装了好多依赖，gem、ruby、jekyll 等等。我觉得每个人的情况不同，我按照教程指引，在配置的过程中同样遇到了教程上所没有提及的问题，但总的来说上面我推荐的教程是比较易于操作的，如若遇到了玄学问题，请自行 Google + baidu ，基本都能解决。&lt;/p&gt;

&lt;p&gt;我是按照&lt;a href=&quot;https://pages.github.com/&quot;&gt;GitHub Pages&lt;/a&gt; + &lt;a href=&quot;http://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt; 快速 Building Blog 的技术方案，非常轻松时尚。&lt;/p&gt;

&lt;p&gt;其优点非常明显：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Markdown&lt;/strong&gt; 带来的优雅写作体验&lt;/li&gt;
  &lt;li&gt;非常熟悉的 Git workflow ，&lt;strong&gt;Git Commit 即 Blog Post&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;利用 GitHub Pages 的域名和免费无限空间，不用自己折腾主机
    &lt;ul&gt;
      &lt;li&gt;如果需要自定义域名，也只需要简单改改 DNS 加个 CNAME 就好了&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Jekyll 的自定制非常容易，基本就是个模版引擎&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我的 jekyll 模板直接 fork 了 &lt;a href=&quot;https://github.com/Huxpro/huxpro.github.io&quot;&gt;Hux Blog&lt;/a&gt;（为大佬喝彩。）&lt;/p&gt;

&lt;p&gt;请按照教程一步一步设置，过程可能会有些繁琐，但小站做好之后幸福感油然而生。&lt;/p&gt;

&lt;h2 id=&quot;后记&quot;&gt;后记&lt;/h2&gt;

&lt;p&gt;如果你恰巧逛到了我的博客，希望你能喜欢。&lt;/p&gt;

&lt;p&gt;—— chenjing 记于2019.8&lt;/p&gt;
</description>
        <pubDate>Sat, 03 Aug 2019 22:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/08/03/Hello-World/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/08/03/Hello-World/</guid>
        
        <category>生活</category>
        
        
      </item>
    
  </channel>
</rss>
