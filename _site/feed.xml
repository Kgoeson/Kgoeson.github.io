<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ChenJing's Blog</title>
    <description>记录 | 陈晶，Web &amp; Mobile Lover，Algrithm Engineer | 这里是 @chenjing 的个人博客，与你一起发现更大的世界。</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sun, 04 Aug 2019 18:58:26 +0800</pubDate>
    <lastBuildDate>Sun, 04 Aug 2019 18:58:26 +0800</lastBuildDate>
    <generator>Jekyll v3.8.6</generator>
    
      <item>
        <title>Stanford CS224n 自然语言处理（一）</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;Begining&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;

&lt;p&gt;我是在4月初入坑自然语言处理的，并在4月初到6月初的2个月时间里面学习了斯坦福的自然语言处理课程。接下来我会大致介绍一下这门课程，讲讲每节课的大纲，以及我觉得缺少的东西。&lt;/p&gt;

&lt;p&gt;首先给出课程的&lt;a href=&quot;http://web.stanford.edu/class/cs224n/&quot;&gt;官方网站&lt;/a&gt;，所有的课程资料都可以在这个网页上下载，包括：课程ppt、作业说明文档、手册、部分笔记等。配套的网络课程在&lt;a href=&quot;https://www.bilibili.com/video/av46065585?from=search&amp;amp;seid=12274272452479951197&quot;&gt;这里&lt;/a&gt;，如果你希望系统地学习 NLP，那么这门课程是你的首选。(6月下旬斯坦福又开放了一门课程 CS224u 自然语言处理，我会另开一个系列介绍)&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;课程大纲&quot;&gt;课程大纲&lt;/h2&gt;

&lt;h4 id=&quot;lecture-1--introduction-and-word-vectors&quot;&gt;Lecture 1 : Introduction and Word Vectors&lt;/h4&gt;

&lt;p&gt;首先介绍了课程的教授者，课程资料以及作业的评分细则，然后列出了和2017年的 CS2224n 课程的不同点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;课程涵盖新的模型和方法，如，字符级模型( Character Model )、Transformer、多任务学习( Multitask Learning )&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;作业涵盖了新内容，如，NMT with attention、卷积网络( ConvNets )、子词模型( subword model )&lt;/li&gt;
  &lt;li&gt;使用 PyTorch 而不是 TensorFlow&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;接着介绍了自然语言以及词义；词向量、word2vec 概览；目标函数梯度的计算以及梯度下降算法。&lt;/p&gt;

&lt;h4 id=&quot;lecture-2--word-vectors-and-word-senses&quot;&gt;Lecture 2 : Word Vectors and Word Senses&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;实现 word2vec 的两种模型
    &lt;ul&gt;
      &lt;li&gt;Skip-grams (SG)&lt;/li&gt;
      &lt;li&gt;Continuous Bag of Words (CBOW)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;训练技巧
    &lt;ul&gt;
      &lt;li&gt;负采样 (Negative sampling)&lt;/li&gt;
      &lt;li&gt;Hierarchical softmax&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;共现矩阵( co-occurrence matrix )，以及降维方法( SVD )&lt;/li&gt;
  &lt;li&gt;GloVe&lt;/li&gt;
  &lt;li&gt;词向量的评价指标&lt;/li&gt;
  &lt;li&gt;词义：歧义( word sense ambiguity )、一词多义( polysemy )等&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-3--word-window-classification-neural-networks-and-matrix-calculus&quot;&gt;Lecture 3 : Word Window Classification, Neural Networks and Matrix Calculus&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;分类问题概览&lt;/li&gt;
  &lt;li&gt;神经网络简介&lt;/li&gt;
  &lt;li&gt;命名实体识别( Named Entity Recognition )&lt;/li&gt;
  &lt;li&gt;词窗口&lt;/li&gt;
  &lt;li&gt;矩阵求导&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-4--backpropagation-and-computation-graphs&quot;&gt;Lecture 4 : Backpropagation and computation graphs&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;推导矩阵反向传播的梯度&lt;/li&gt;
  &lt;li&gt;计算图( computation graphs )&lt;/li&gt;
  &lt;li&gt;神经网络的一些需要知道的问题：
    &lt;ul&gt;
      &lt;li&gt;向量化表达( Vectorization ) / 矩阵化表达( Matrixization )的计算速度优势&lt;/li&gt;
      &lt;li&gt;过拟合( overfitting )&lt;/li&gt;
      &lt;li&gt;非线性( nonlinearities )&lt;/li&gt;
      &lt;li&gt;初始化( initialization )&lt;/li&gt;
      &lt;li&gt;优化器( optimizers )&lt;/li&gt;
      &lt;li&gt;学习率( learning rates )&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-5--dependency-parsing&quot;&gt;Lecture 5 : Dependency Parsing&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;两种句法结构：
    &lt;ul&gt;
      &lt;li&gt;一致性( Consistency ) ，即短语结构语法，也即无上下文语法( context-free grammars )，18课会单独讲&lt;/li&gt;
      &lt;li&gt;依存性( Dependency )，即依存结构&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;依存语法和依存结构&lt;/li&gt;
  &lt;li&gt;依存关系解析( Dependency Parsing )
    &lt;ul&gt;
      &lt;li&gt;基于转换的解析( transition-based )&lt;/li&gt;
      &lt;li&gt;基于神经网络的解析( nueral )&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-6--language-models-and-recurrent-neural-networks&quot;&gt;Lecture 6 : Language Models and Recurrent Neural Networks&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;语言模型简介&lt;/li&gt;
  &lt;li&gt;n-gram 语言模型&lt;/li&gt;
  &lt;li&gt;神经网络（RNN）语言模型&lt;/li&gt;
  &lt;li&gt;评估语言模型&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-7--vanishing-gradients-and-fancy-rnns&quot;&gt;Lecture 7 : Vanishing Gradients and Fancy RNNs&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;梯度消失问题和梯度爆炸问题及其解决方法&lt;/li&gt;
  &lt;li&gt;LSTM （ Long Short-Term Memory ）&lt;/li&gt;
  &lt;li&gt;GRU（ Gated Recurrent Unit ）&lt;/li&gt;
  &lt;li&gt;RNN variant：
    &lt;ul&gt;
      &lt;li&gt;Bidirectional RNN&lt;/li&gt;
      &lt;li&gt;Multi-layer RNN&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-8--machine-translation-sequence-to-sequence-and-attention&quot;&gt;Lecture 8 : Machine Translation, Sequence-to-sequence and Attention&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;统计机器翻译（ SMT ）
    &lt;ul&gt;
      &lt;li&gt;对齐（alignment）&lt;/li&gt;
      &lt;li&gt;解码（decodeing）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;神经机器翻译（ NMT ）
    &lt;ul&gt;
      &lt;li&gt;seq2seq&lt;/li&gt;
      &lt;li&gt;beam search decoding&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Attention 机制&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-9--practical-tips-for-final-projects&quot;&gt;Lecture 9 : Practical Tips for Final Projects&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;最终项目的一些建议&lt;/li&gt;
  &lt;li&gt;回顾 GRU、LSTM&lt;/li&gt;
  &lt;li&gt;机器翻译的评价指标：BLEU&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-10--textual-question-answering&quot;&gt;Lecture 10 : (Textual) Question Answering&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;QA —&amp;gt;机器阅读理解&lt;/li&gt;
  &lt;li&gt;SQuAD 数据集&lt;/li&gt;
  &lt;li&gt;斯坦福 Attentive Reader&lt;/li&gt;
  &lt;li&gt;BiDAF&lt;/li&gt;
  &lt;li&gt;一些先进的模型：Co-attention、FusionNet 等&lt;/li&gt;
  &lt;li&gt;ELMo 和 BERT 概览&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-11--convnets-for-nlp&quot;&gt;Lecture 11 : ConvNets for NLP&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;从 RNN 到 CNN&lt;/li&gt;
  &lt;li&gt;模型比较&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-12--information-from-parts-of-words--subword-models&quot;&gt;Lecture 12 : Information from parts of words : Subword Models&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;语言学概览
    &lt;ul&gt;
      &lt;li&gt;发音（Phonetics）&lt;/li&gt;
      &lt;li&gt;音韵（Phonology）&lt;/li&gt;
      &lt;li&gt;词的形态（Morphology）&lt;/li&gt;
      &lt;li&gt;书写系统&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;字符级的模型&lt;/li&gt;
  &lt;li&gt;子词模型
    &lt;ul&gt;
      &lt;li&gt;Byte Pair Encoding&lt;/li&gt;
      &lt;li&gt;Wordpiece&lt;/li&gt;
      &lt;li&gt;Sentencepiece&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;混合神经网络翻译&lt;/li&gt;
  &lt;li&gt;fastText&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-13--contextual-word-representations-and-pretraining&quot;&gt;Lecture 13 : Contextual Word Representations and Pretraining&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;词表示&lt;/li&gt;
  &lt;li&gt;pre-ELMo &amp;amp; EMLo&lt;/li&gt;
  &lt;li&gt;ULMfit&lt;/li&gt;
  &lt;li&gt;Transformer 架构
    &lt;ul&gt;
      &lt;li&gt;self-attention&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;BERT&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-14--self-attention-for-generative-models&quot;&gt;Lecture 14 : Self-Attention for Generative Models&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;Self-Attention&lt;/li&gt;
  &lt;li&gt;文本生成&lt;/li&gt;
  &lt;li&gt;图像生成&lt;/li&gt;
  &lt;li&gt;音乐生成&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-15--natural-language-generation&quot;&gt;Lecture 15 : Natural Language Generation&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;回顾语言模型和解码算法及技巧
    &lt;ul&gt;
      &lt;li&gt;Greedy decoding&lt;/li&gt;
      &lt;li&gt;Beam search&lt;/li&gt;
      &lt;li&gt;Sampling methods&lt;/li&gt;
      &lt;li&gt;Softmax temperature&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;自然语言生成（NLG）任务以及神经网络方法
    &lt;ul&gt;
      &lt;li&gt;文本总结&lt;/li&gt;
      &lt;li&gt;对话系统&lt;/li&gt;
      &lt;li&gt;创意写作&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;NLG 的评估&lt;/li&gt;
  &lt;li&gt;NLG 的趋势和发展&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-16--coreference-resolution&quot;&gt;Lecture 16 : Coreference Resolution&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;共指解析及其应用&lt;/li&gt;
  &lt;li&gt;提及检测（Mention Detection）&lt;/li&gt;
  &lt;li&gt;一些语言学概念
    &lt;ul&gt;
      &lt;li&gt;anaphora&lt;/li&gt;
      &lt;li&gt;cataphora&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;共指模型
    &lt;ul&gt;
      &lt;li&gt;Rule-based Model&lt;/li&gt;
      &lt;li&gt;Mention Pair Model&lt;/li&gt;
      &lt;li&gt;Mention Ranking Model&lt;/li&gt;
      &lt;li&gt;Mention Clustering Model&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;评估指标&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-17--multitask-learning-as-qa&quot;&gt;Lecture 17 : Multitask Learning as QA&lt;/h4&gt;

&lt;p&gt;这节课相当于一个讲座，介绍了多任务学习的若干模型、训练策略，以及多任务学习的优势&lt;/p&gt;

&lt;h4 id=&quot;lecture-18--tree-recursive-neural-networks-constituency-parsing-and-sentiment&quot;&gt;Lecture 18 : Tree Recursive Neural Networks, Constituency Parsing, and Sentiment&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;递归神经网络（Recursive Neural Networks）&lt;/li&gt;
  &lt;li&gt;短语句法解析&lt;/li&gt;
  &lt;li&gt;树递归神经网络（TreeRNN）&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-19--bias-in-the-vision-and-language-of-artificial-intelligence&quot;&gt;Lecture 19 : Bias in the Vision and Language of Artificial Intelligence&lt;/h4&gt;

&lt;p&gt;讲座，介绍了 AI 的偏见以及多任务学习&lt;/p&gt;

&lt;h4 id=&quot;lecture-20--the-future-of-deep-learning--nlp&quot;&gt;Lecture 20 : The Future of Deep Learning + NLP&lt;/h4&gt;

&lt;p&gt;讲座，介绍了大规模深度学习在各个领域取得的成功，无标签数据集，无监督机器翻译，大模型（如 BERT、GPT-2），多任务学习，更加难的自然语言理解，NLP 在工业上的应用，以及未来的挑战。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;后记&quot;&gt;后记&lt;/h2&gt;

&lt;p&gt;以上就是Stanford CS224n 自然语言处理课程的课程概览，我会在今后陆续完成每节课的分析与笔记（如果有时间的话 hhh）。&lt;/p&gt;

&lt;p&gt;自然语言处理真的是一门非常庞大的学科，其中包含了很多内容，想要在一门课上学习完所有的内容是不现实的，个人所见，这门课的的意义就在于为刚开始学习自然语言处理的学生做一个相对详细的 review ，接下去要做什么方向，按照自己的兴趣来。&lt;/p&gt;

&lt;p&gt;在前言中我提到过，这门课程还有许多没有讲到的地方，比如分词方法、TF-IDF、关系提取、情感分析等，如果对这些感兴趣的话，Stanford 为大家又提供了一门课程（&lt;a href=&quot;http://web.stanford.edu/class/cs224u/&quot;&gt;CS224u 自然语言理解&lt;/a&gt;），感谢斯坦福。&lt;/p&gt;

&lt;p&gt;写这些文章的初衷是为了对抗遗忘，所以某种程度上是给自己温故用的。所以不会把课上的内容完完整整全部记录下来，如果有什么疑问，欢迎指出。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PS&lt;/strong&gt; ：博客的评论区还没有调试成功，如果我的文章有什么错误，烦请点击下面的微博按钮，私信给我，不胜感激。&lt;/p&gt;

&lt;hr /&gt;

</description>
        <pubDate>Sun, 04 Aug 2019 23:04:00 +0800</pubDate>
        <link>http://localhost:4000/2019/08/04/Stanford-CS224n-nlp-01/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/08/04/Stanford-CS224n-nlp-01/</guid>
        
        <category>NLP</category>
        
        <category>Stanford</category>
        
        
      </item>
    
      <item>
        <title>Hello World</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;“Yeah It’s on.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;

&lt;p&gt;我的Blog就这么开通了。&lt;/p&gt;

&lt;p&gt;鼓捣了两天，不容易啊。。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#build&quot;&gt;这里推荐几个写的比较好的教程&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;其实很早就想建一个小站，记录一下学习和生活，因为自己记性不好，总想着留下点印记，以便回溯，那么就从今日开始吧。&lt;/p&gt;

&lt;p id=&quot;build&quot;&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;a href=&quot;http://bluebiu.com/blog/create-blog-in-github.html&quot;&gt;从零开始用GitHub Pages创建博客&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;[http://bluebiu.com/blog/learn-to-use-jekyll.html#12%E7%AE%80%E5%8D%95%E5%A5%97%E7%94%A8](http://bluebiu.com/blog/learn-to-use-jekyll.html#12简单套用)&quot;&gt;从零开始折腾Jekyll&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://ambeta.github.io/2016/03/27/build-website-with-jekyll.html#jekyll-template&quot;&gt;如何使用jekyll建站&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://chaosinmotion.coding.me/cblog/2016/03/26/build-a-blog/&quot;&gt;Github Pages + Jekyll 独立博客一小时快速搭建&amp;amp;上线指南&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;正文&quot;&gt;正文&lt;/h2&gt;

&lt;p&gt;接下来说说搭建这个博客的过程。&lt;/p&gt;

&lt;p&gt;我的操作系统是macOS，一开始装了好多依赖，gem、ruby、jekyll 等等。我觉得每个人的情况不同，我按照教程指引，在配置的过程中同样遇到了教程上所没有提及的问题，但总的来说上面我推荐的教程是比较易于操作的，如若遇到了玄学问题，请自行 Google + baidu ，基本都能解决。&lt;/p&gt;

&lt;p&gt;我是按照&lt;a href=&quot;https://pages.github.com/&quot;&gt;GitHub Pages&lt;/a&gt; + &lt;a href=&quot;http://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt; 快速 Building Blog 的技术方案，非常轻松时尚。&lt;/p&gt;

&lt;p&gt;其优点非常明显：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Markdown&lt;/strong&gt; 带来的优雅写作体验&lt;/li&gt;
  &lt;li&gt;非常熟悉的 Git workflow ，&lt;strong&gt;Git Commit 即 Blog Post&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;利用 GitHub Pages 的域名和免费无限空间，不用自己折腾主机
    &lt;ul&gt;
      &lt;li&gt;如果需要自定义域名，也只需要简单改改 DNS 加个 CNAME 就好了&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Jekyll 的自定制非常容易，基本就是个模版引擎&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我的 jekyll 模板直接 fork 了 &lt;a href=&quot;https://github.com/Huxpro/huxpro.github.io&quot;&gt;Hux Blog&lt;/a&gt;（为大佬喝彩。）&lt;/p&gt;

&lt;p&gt;请按照教程一步一步设置，过程可能会有些繁琐，但小站做好之后幸福感油然而生。&lt;/p&gt;

&lt;h2 id=&quot;后记&quot;&gt;后记&lt;/h2&gt;

&lt;p&gt;如果你恰巧逛到了我的博客，希望你能喜欢。&lt;/p&gt;

&lt;p&gt;—— chenjing 记于2019.8&lt;/p&gt;
</description>
        <pubDate>Sat, 03 Aug 2019 22:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/08/03/Hello-World/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/08/03/Hello-World/</guid>
        
        <category>生活</category>
        
        
      </item>
    
  </channel>
</rss>
