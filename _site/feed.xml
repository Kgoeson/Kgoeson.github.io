<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ChenJing's Blog</title>
    <description>记录 | 陈晶，Web &amp; Mobile Lover，Algrithm Engineer | 这里是 @chenjing 的个人博客，与你一起发现更大的世界。</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 15 Aug 2019 20:11:33 +0800</pubDate>
    <lastBuildDate>Thu, 15 Aug 2019 20:11:33 +0800</lastBuildDate>
    <generator>Jekyll v3.8.6</generator>
    
      <item>
        <title>Stanford CS224n 自然语言处理（二）</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;You shall know a word by the company it keeps.						—— J. R. Firth&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;0-前言&quot;&gt;0 前言&lt;/h1&gt;

&lt;p&gt;这篇文章涵盖 CS224n 的 Lecture 1 和 Lecture 2 中的部分内容。抛却开头的课程介绍，我们直入正题。主要内容包括：自然语言以及词义；词向量；共现矩阵( co-occurrence matrix ) ；降维方法( SVD )；word2vec 概览；实现 word2vec 的两种模型：Skip-Gram 和 Continuous Bag-of-Word。&lt;/p&gt;

&lt;p&gt;因为是第一篇正式介绍内容的文章，概念有点多，中途遇到看不懂的地方可以先略过，可能在后面会有解释，再返回去看，就会串起来。&lt;/p&gt;

&lt;h1 id=&quot;1-自然语言处理介绍&quot;&gt;1 自然语言处理介绍&lt;/h1&gt;

&lt;h2 id=&quot;11-自然语言&quot;&gt;1.1 自然语言&lt;/h2&gt;

&lt;p&gt;什么是自然（人类）语言？狭义地来说，自然语言通常是指一种自然地随文化演化的语言，即我们所知的汉语、英语、日语等。广义地来说，自然语言还包括人造语言，比如世界语。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;此处的“人造语言” 世界语，区别于编程语言等人造语言。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;人类语言是一个专门用来表达意义（meaning）的系统，而不是由任何物理表现产生的。以这种方式来说，它与视觉或任何形式的机器学习任务都不同。也就是说，图像是一种物理表现，这很好理解，日常生活中的照片，画册，亦或是计算机中的 RGB 色图，就是图像的物理表现，脱离了这种传递信息的介质，图像便不能很好地传递信息。而自然语言可以脱离这种方式，以自身直接传递信息，一般来说，你只需要通过若干个单词的描述，就能够准确无误地表达意义。&lt;/p&gt;

&lt;p&gt;大多数单词只是一个超语言实体（extra-linguistic entity）的符号：单词是一种符号，该符号映射到一个想法或事物。这种映射也称作指代语义（Denotational semantics）。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;signifier ( symbol )  $\Longleftrightarrow$  signified ( idea or thing )&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;signifier:&lt;/strong&gt; a sign’s physical form (such as a sound, printed word, or image) as distinct from its meaning.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;signified:&lt;/strong&gt; the meaning or idea expressed by a sign, as distinct from the physical form in which it is expressed.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;举个例子。&lt;strong&gt;signifier ( symbol )  $\Rightarrow$  signified ( idea or thing )&lt;/strong&gt; ，就是在纸上写着“椅子”（符号），你就能联想到具体的1号椅子，2号椅子…。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;signifier ( symbol )  $\Leftarrow$  signified ( idea or thing )&lt;/strong&gt;，指着具体的椅子给你看，你能抽象出椅子这一记号。&lt;/p&gt;

&lt;p&gt;还有，这些语言的符号可以被编码成几种形式：声音、手势、文字等。然后通过&lt;em&gt;连续&lt;/em&gt;信号传输给大脑，大脑本身似乎也能以一种连续的方式编码这些信号。&lt;/p&gt;

&lt;p&gt;那么，我们现在已经知道我们要 &lt;em&gt;“处理”&lt;/em&gt; 的对象了，那么该如何处理呢？这就是 CS224n 这个系列的主题，接下去的一系列文章将会带你领略 &lt;strong&gt;自然语言处理&lt;/strong&gt; 这门融语言学、计算机科学、数学于一体的学科。&lt;/p&gt;

&lt;h2 id=&quot;12-自然语言处理的任务&quot;&gt;1.2 自然语言处理的任务&lt;/h2&gt;

&lt;p&gt;自然语言处理的 目标 是&lt;strong&gt;通过设计算法来使计算机能够“理解”语言，从而能够执行某些特定的任务&lt;/strong&gt;，而不同的任务的难度是不一样的：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;简单：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;拼写检查&lt;/li&gt;
  &lt;li&gt;关键词搜索&lt;/li&gt;
  &lt;li&gt;同义词查找&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;中等：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;解析来自网站文档等的信息&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;困难：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;机器翻译&lt;/li&gt;
  &lt;li&gt;语义分析（例如，“陈述”这个词是什么意思？）&lt;/li&gt;
  &lt;li&gt;共指（例如，“他”和“它”在文档中分别指代什么？）&lt;/li&gt;
  &lt;li&gt;问答系统&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;2-词的表示&quot;&gt;2 词的表示&lt;/h1&gt;

&lt;h2 id=&quot;21-如何表示词&quot;&gt;2.1 如何表示词&lt;/h2&gt;

&lt;p&gt;那么我们该如何使计算机明白一个词、一句话，乃至一篇文章的意思呢？&lt;/p&gt;

&lt;p&gt;我们先举个实际例子，大家或许都知道 WorldNet ，他是自然语言处理工具包 NLTK 包含一个同义词集（synonym sets）和上位词（hypernyms）的词库。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;hypernyms :&lt;/strong&gt;  “is a” 的关系。如，Rose is a flower。Flower is a plant。即 ”花” 是 ”玫瑰” 的上位词，”植物” 是 ”花” 的上位词。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cs224n/worldnet_demo.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到它是一个可以很好地进行同义词比对和上位词查找的词库。那么，像这种基于人工统计的词库有没有缺点呢？答案是肯定的。&lt;/p&gt;

&lt;p&gt;首先，它不能判别词的细微差别，如，“proficient” 是 “good” 的同义词，但是在一些文章中并不是。第二，它缺少了很多新词，如，badass, nifty, wizard, ninja 等，再进一步，通过 WorldNet 的组织形式可以看出，要加入新词几乎是不可操作的。第三，由于是人工统计的，显得太主观。第四，需要大量的人力和物力去创建和维护。还有，它不能定量地计算词与词之间的相似度。&lt;/p&gt;

&lt;p&gt;为了让大多数的自然语言处理任务能有更好的表现，我们先需要了解单词之间的相似和不同，于是词向量应运而生。&lt;/p&gt;

&lt;h2 id=&quot;22-词向量&quot;&gt;2.2 词向量&lt;/h2&gt;

&lt;p&gt;在所有的 NLP 任务中，最重要的是我们如何将单词表示为任意模型的输入。而模型的输入是一组数字，于是很自然地想到用向量来表示一个词，即把单词映射为实数向量，这种向量就叫做词向量。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;词向量（word vector）&lt;/strong&gt; 有时又被称作&lt;strong&gt;词嵌入 （word embedding）&lt;/strong&gt; 或者&lt;strong&gt;词表示（word representation）&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;一共有1千3百万个左右的英语单词，它们其中很多都是有关系的。例如 “feline” 和 “cat”，“hotel” 和 “motel” 。因此，我们希望用词向量编码单词使它嵌入到词组空间中，代表其中的一个点（这也是词向量称为词嵌入的原因）。这样做最直观的原因是，在实际中可能存在 $N$ 维空间（$N$ $\ll$ $13 million$）足以编码所有单词以及语义。每个维度都会编码一些使用言语传达的意思。例如，语义维度可能表示时态（过去、现在和未来），计数（单数和复数）和性别（男性和女性）。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;这里先提前区别几组名词：&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;distributed representation&lt;/strong&gt; (密集型表示) 与 &lt;strong&gt;symbolic representation&lt;/strong&gt;（localist representation、one-hot representation）相对。&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;discrete representation&lt;/strong&gt; (离散表示) 与 &lt;strong&gt;symbolic representation&lt;/strong&gt; (符号表示) 及 &lt;strong&gt;denotation&lt;/strong&gt; 的意思相似。&lt;/p&gt;

  &lt;p&gt;切不可搞混 &lt;strong&gt;distributed&lt;/strong&gt; 和 &lt;strong&gt;discrete&lt;/strong&gt; 这两个词。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我们从最简单的开始，传统的 NLP 的做法是将词看作是一组离散的符号，将词表示成 &lt;strong&gt;one-hot 向量&lt;/strong&gt;：每个词都是一个 $\mathbb{R}^{\mid V\mid \times 1}$ 向量，其中除了该单词所在的索引为 1 外其他索引都是 0。在这个定义下， $\mid V\mid$ 是词汇表的大小，即词表中词的个数。这时词向量的可以表示为&lt;/p&gt;

&lt;p&gt;$hotel$  = [ 0 0 0 0 1 0 0 ··· 0 ]&lt;/p&gt;

&lt;p&gt;$motel$ = [ 0 1 0 0 0 0 0 ··· 0 ]&lt;/p&gt;

&lt;p&gt;$cat$    = [ 0 0 0 1 0 0 0 ··· 0 ]&lt;/p&gt;

&lt;p&gt;但是，这样的表示无法给出词之间的相似性，因为根据相似度的一种定义（内积）：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left(w^{\text { hotel }}\right)^{T} w^{\text { motel }}=\left(w^{\text { hotel }}\right)^{T} w^{\text { cat }}=0&lt;/script&gt;

&lt;p&gt;很不可思议吧，$hotel$ 与 $motel$ 的相似度竟然等于 $hotel$ 和 $cat$ 的相似度。这是因为维数越大，数据越稀疏，比较相似度往往缺乏实际意义。&lt;/p&gt;

&lt;p&gt;那么接下来很明确，就是降低维度，使数据变得稠密，找到一个更低维度的向量空间来编码词与词之间的关系。&lt;/p&gt;

&lt;h2 id=&quot;23-共现矩阵&quot;&gt;2.3 共现矩阵&lt;/h2&gt;

&lt;p&gt;在介绍降维的方法之前，我先引入共现矩阵（co-occurrence）的概念。共现矩阵也叫共现计数矩阵，是基于统计方法得到的矩阵，记作 $X$。&lt;/p&gt;

&lt;h3 id=&quot;231-基于文档的共现矩阵&quot;&gt;2.3.1 基于文档的共现矩阵&lt;/h3&gt;

&lt;p&gt;我们猜想，有关联的词经常会出现在同一个文档中，例如，“banks”，“stocks”，“shares” 等，出现在同一篇的文档的概率较高，而 “banks”，“banana”，“phone”出现在同一篇文档的概率较小。根据这种情况，我们可以建立一个词-文档矩阵 $X$ ，$X$ 按照以下方式构建：遍历文档，当词 $i$ 出现在文档 $j$ ，我们对 $X_{ij}$ 加一，遍历结束，我们便可得到一个词-文档矩阵。但这显然是一个很大的矩阵$X\in\mathbb{R}^{\mid V\mid \times M}$ ，它的规模是和文档数 $M$ 成正比的，而 $M$ 通常非常大（$billion$级），因此，不考虑这种方式。&lt;/p&gt;

&lt;h3 id=&quot;232-基于窗口的共现矩阵&quot;&gt;2.3.2 基于窗口的共现矩阵&lt;/h3&gt;

&lt;p&gt;使用基于窗口的方式，我们直接通过单词与单词之间的共现来计算共现矩阵。在这种方法中，我们统计每个单词在感兴趣单词的附近特定大小的窗口中出现的次数。我们按照这个方法对语料库中的所有单词进行统计。当窗口大小为2时，窗口是这样移动的，直至最后一个单词。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cs224n/window_based.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;举个例子，语料库由三句话组成，窗口的大小是 1（考虑中心词左右两侧的第一个单词）：&lt;/p&gt;

&lt;p&gt;I enjoy flying.&lt;/p&gt;

&lt;p&gt;I like NLP.&lt;/p&gt;

&lt;p&gt;I like deep learning.&lt;/p&gt;

&lt;p&gt;共现矩阵如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cs224n/co_occurrence_matrix.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;24-svd&quot;&gt;2.4 SVD&lt;/h2&gt;

&lt;p&gt;这里先提两个概念：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Denotational semantics:&lt;/strong&gt; The concept of representing an idea as a symbol (a word or a one-hot vector). It is sparse and cannot capture similarity. This is a “localist” representation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Distributional semantics:&lt;/strong&gt; The concept of representing the meaning of a word based on the context in which it usually appears. It is dense and can better capture similarity.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;简而言之，就是 &lt;strong&gt;Denotational semantics&lt;/strong&gt; 是用符号表示词，稀疏，无法获得相似性，而 &lt;strong&gt;Distributional semantics&lt;/strong&gt; 是用上下文表示词，稠密，可以很好地获取相似性。我们在上面说的 one-hot 向量表示词就是一种 &lt;strong&gt;Denotational semantics&lt;/strong&gt;，而以下要说的 SVD 方法就是一种  &lt;strong&gt;Distributional semantics&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;SVD即奇异值分解，是矩阵分解的一种。SVD 方法是一种找到词向量的方法，首先遍历一个很大的数据集和统计词的共现计数矩阵 $X$，然后对矩阵 $X$ 进行 SVD 分解得到 $USV^{T}$ （$U,S,V\in\mathbb{R}^{\mid V\mid \times \mid V\mid}$）。通过选择前 $k$ 个奇异值来降低维度，然后使用 $U_{1:\mid V\mid,1:k} \in \mathbb{R}^{\mid V\mid \times k}$ 的行向量作为词汇表中所有词的词向量，词向量的维度为 $k$。&lt;/p&gt;

&lt;p&gt;SVD 方法能让我们的词向量编码充分的语义和句法（词性标注）的信息，但是也会存在许多问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;共现矩阵的维度会经常发生改变（经常增加新的单词和语料库的大小会改变）。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;共现矩阵会非常的稀疏，因为很多词不会共现。&lt;/li&gt;
  &lt;li&gt;共现矩阵的维度一般会非常高（$ \approx 10^{6} \times 10^{6}$ ）。&lt;/li&gt;
  &lt;li&gt;基于 SVD 的方法的计算复杂度一般为 $O\left(mn^{2}\right)$ 。&lt;/li&gt;
  &lt;li&gt;需要在 $X$ 上加入一些技巧处理来解决词频的不平衡。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对上述讨论中存在的问题有以下的解决方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;忽略功能词，例如 “the”，“he”，“has” 等等。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;使用 ramp window ，即根据中心词与上下文词之间的距离远近，赋予共现计数不同的权重。&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;如，和中心词最相邻的上下文词的计数权重为1，相隔5个位置的计数权重为0.5。&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;使用皮尔逊相关系数将负数的计数设为 0，而不是使用原始的计数。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在下一部分，基于迭代的方法可以用一种更为优雅的方式解决大部分上述问题。&lt;/p&gt;

&lt;h1 id=&quot;3-word2vec&quot;&gt;3 word2vec&lt;/h1&gt;

&lt;p&gt;我们回顾一下文章开头的话：You shall know a word by the company it keeps。这是 J. R. Firth 大佬说的，他是现代统计自然语言处理最成功的思想之一。简单来说，就是我们要到具体的上下文当中去理解一个词。上一节中提出的共现矩阵其实就是一种捕捉单词上下文词信息的方法。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cs224n/context_word.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;word2vec&lt;/strong&gt; 是一种从大量文本语料中学习语义知识的模型，它被大量地用在自然语言处理（NLP）中。它的模型参数就是词向量，用词向量表征词的语义信息，词向量可作为下游任务的输入。&lt;/p&gt;

&lt;p&gt;word2vec 模型实际上分为了两个部分，第一部分为建立模型，第二部分是通过模型获取嵌入词向量。word2vec的整个建模过程实际上与自编码器（auto-encoder）的思想很相似，即先基于训练数据构建一个神经网络，当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵——后面我们将会看到这些权重在 word2vec 中实际上就是我们试图去学习的“词向量”。基于训练数据建模的过程，我们给它一个名字叫“Fake Task”，意味着建模并不是我们最终的目的。&lt;/p&gt;

&lt;p&gt;word2vec 模型主要有 Skip-Gram 和 Continuous Bag-of-Word 两种模型，从直观上理解，Skip-Gram 是给定中心词（center word）来预测上下文词（context word），而 CBOW 是给定上下文词（context word），来预测中心词（center word）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cs224n/CBOW_arc.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;center&gt;CBOW 模型&lt;/center&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cs224n/SG_arc.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;center&gt;Skip-Gram 模型&lt;/center&gt;

&lt;h2 id=&quot;31-language-model&quot;&gt;3.1 Language Model&lt;/h2&gt;

&lt;p&gt;在正式介绍 word2vec 模型之前，我们需要了解一些语言模型（language model）相关的概念，我们从一个简单的例子开始：&lt;/p&gt;

&lt;p&gt;“ The cat jumped over the puddle. ”​&lt;/p&gt;

&lt;p&gt;一个好的语言模型会给这个句子很高的概率，因为在句法和语义上这是一个完全有效的句子。相似地，句子 “ stock boil fish is toy. ” 会得到一个很低的概率，因为这是一个无意义的句子。在数学上，我们可以称给定 n 个词的序列的概率是：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P\left(w_{1}, w_{2}, \cdots, w_{n}\right)&lt;/script&gt;

&lt;p&gt;我们可以使用 unigram 语言模型方法，通过假设单词的出现是符合独立同分布假设的：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P\left(w_{1}, w_{2}, \cdots, w_{n}\right)=\prod_{i=1}^n P\left(w_{i}\right)&lt;/script&gt;

&lt;p&gt;但是我们知道这是不合理的，因为下一个单词是高度依赖于前面的单词序列的。如果使用上述的语言模型，可能会让一个无意义的句子具有很高的概率（只需要选取那些出现频率高的单词组成一个句子）。所以我们可以让序列的概率 等于 序列中每个单词和其旁边的单词组成单词对的概率的乘积。我们称之为 bigram 模型：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P\left(w_{1}, w_{2}, \cdots, w_{n}\right)=\prod_{i=2}^{n} P\left(w_{i} | w_{i-1}\right)&lt;/script&gt;

&lt;p&gt;虽然这个方法会有提升，但还是过于简单，不能捕获完整的语义信息，因为我们只关心一对邻近的单词，而不是针对整个句子来考虑。考虑在词-词共现矩阵中，共现窗口为 1，我们基本上能得到这样的成对的概率。但是，这需要计算和存储大量数据集的全局信息。&lt;/p&gt;

&lt;p&gt;类似的方法还有 trigram 模型、four-gram 模型等，统称为 n-gram 模型。这些模型的计算非常复杂，在实际情况中最多取 four-gram，原则上能用 trigram 解决的问题绝对不用 four-gram。&lt;/p&gt;

&lt;p&gt;现在我们知道该如何计算一个序列的概率，接下来就是 word2vec 中一些可以计算这些概率的模型。使用迭代的方法简化模型使得训练模型速度非常快，而不是像 n-gram 模型一样去计算和维护一个庞大数据集得到全局信息。&lt;/p&gt;

&lt;h2 id=&quot;32-the-fake-task&quot;&gt;3.2 The Fake Task&lt;/h2&gt;

&lt;h3 id=&quot;321-任务介绍&quot;&gt;3.2.1 任务介绍&lt;/h3&gt;

&lt;p&gt;我们在介绍 wordvec 的时候提到，训练模型的真正目的是获得模型基于训练数据学得的隐层权重。为了得到这些权重，我们首先要构建一个完整的神经网络作为我们的“Fake Task”，后面再返回来通过“Fake Task”间接地得到这些词向量。&lt;/p&gt;

&lt;p&gt;接下来我们来看看如何训练我们的神经网络。假如我们有一个句子： $\text {“The dog barked at the mailman”}$。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;首先我们选句子中间的一个词作为我们的输入词，例如我们选取“dog”作为中心词；&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;这里做一个规定：对于 CBOW 模型，input word 是上下文词，output word 是中心词，而对于 Skip-Gram 模型，input word 是中心词，output word 是上下文词。标记有点多，千万不要混淆。&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;我们再定义一个 window_size 参数，它代表着我们从当前中心词的一侧（左边或右边）选取词的数量。如果我们设置 window_size = 2 ，那么我们最终获得窗口中的词（包括中心词在内）就是[‘The’, ‘dog’，’barked’, ‘at’]。window_size = 2 代表着选取中心词左侧2个词和右侧2个词进入我们的窗口，所以整个窗口大小 span = 4。另一个参数叫 num_skips，它代表着我们从整个窗口中选取多少个&lt;strong&gt;不同的词&lt;/strong&gt;作为我们的 output word，当 window_size = 2，num_skips = 2 时，我们将会得到两组 (input word, output word) 形式的训练数据，即 (‘dog’, ‘barked’)，(‘dog’, ‘the’)。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;神经网络基于这些训练数据将会输出一个概率分布，这个概率代表在给定 input word 的情况下词表中的每个词出现的概率。再用交叉熵损失将这个概率分布与 output word 的 one-hot 向量的误差反向传播。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;模型的输出概率代表词表中每个词有多大可能性跟 input word 同时出现。举个栗子，如果我们向神经网络模型中输入一个单词“Soviet“，那么最终模型的输出概率中，像“Union”， ”Russia“这种相关词的概率将远高于像”watermelon“，”kangaroo“非相关词的概率。因为”Union“，”Russia“更有可能在”Soviet“的窗口中出现。
我们将通过给神经网络输入文本中成对的单词来训练它，完成上面所说的概率计算。下面的图中给出了一些我们的训练样本的例子。&lt;/p&gt;

&lt;p&gt;假设有句子：$\text{ “The quick brown fox jumps over lazy dog. ”}$ ，设定我们的窗口大小为2（window_size = 2）。下图中，蓝色代表input word，方框内代表位于窗口内的单词。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cs224n/training_data.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;模型将会从每对单词出现的次数中学习得到统计结果。例如，我们的神经网络可能会得到更多类似（“Soviet“，”Union“）这样的训练样本对，而（”Soviet“，”Sasquatch“）这样的训练样本对却很少。因此，当我们的模型完成训练后，给定一个单词”Soviet“作为输入，输出的结果中”Union“或者”Russia“要比”Sasquatch“的概率大得多。&lt;/p&gt;

&lt;h3 id=&quot;322--任务实施&quot;&gt;3.2.2  任务实施&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;1  输入层&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;我们首先从最简单情形开始，input word 只有一个，即给定一个 input word，预测 output word（有点像 bigram 模型）。假设词表大小为$V$，隐藏层神经元个数为$N$，输入层到隐藏层、隐藏层到输出层都是全连接，样本是（input word，output word）单词对，输入为 input word 的 one-hot 向量。&lt;/p&gt;

&lt;p&gt;假设从我们的训练文档中抽取出 10000 个不重复的单词组成词汇表。我们对这 10000 个单词进行 one-hot 编码，得到的每个单词都是一个 10000 维的向量，向量每个维度的值只有 0 或者 1，假如单词 “ants“ 在词汇表中的出现位置为第 3 个，那么 ‘’ants“ 的向量就是一个第三维度取值为 1，其他维都为 0 的 10000 维的向量（$\text {ants}=[0,0,1,0, \ldots, 0]$ ）&lt;/p&gt;

&lt;p&gt;模型的输入如果为一个 10000 维的向量，那么输出也是一个 10000 维度（词汇表的大小）的向量，它包含了 10000 个概率，每一个概率代表着给定 input word 词表中的每个词出现在 input word 附近的概率大小。&lt;/p&gt;

&lt;p&gt;下图是我们神经网络的结构：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cs224n/one_word_net_arch.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;隐层没有使用任何激活函数，但是输出层使用了sotfmax。&lt;/p&gt;

&lt;p&gt;我们使用成对的单词来对神经网络进行训练，训练样本是 ( input word, output word ) 这样的单词对，input word 和 output word 都是 one-hot 编码的向量。最终模型的输出是一个概率分布，每一个输出神经元都是一个概率值，表示在给定 input word 的情况下词表中每个词出现的概率，即 $p\left(w_{j} \mid w_{I}\right)$，$w_{I}$ 表示 input word，$w_{j}$，表示词表中的词，$\text j=1,2, \ldots, V$ 。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2  隐藏层&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;说完单词的编码和训练样本的选取，我们来看下我们的隐层。如果我们现在想用300个特征来表示一个单词（即每个词可以被表示为300维的向量）。那么隐层的权重矩阵应该为10000行（词汇表中的每个单词），300列（每个隐藏神经元）。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Google在最新发布的基于Google news数据集训练的模型中使用的就是300维的词向量。词向量的维度是一个可以调节的超参数，可以根据不同的任务调节，实践中一般300维为佳。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;看下面的图片，左右两张图分别从不同角度代表了输入层-隐层的权重矩阵。左图中每一列代表一个10000维的词向量和隐层单个神经元连接的权重向量。从右边的图来看，每一行实际上代表了每个单词的词向量。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cs224n/word2vec_weight_matrix_lookup_table.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;现在我们已经完成了“Fake Taks”，我们最终的目标就是学习到隐层的权重矩阵。&lt;/p&gt;

&lt;p&gt;我们现在回来，接着通过模型的定义来训练我们的这个模型。&lt;/p&gt;

&lt;p&gt;上面我们提到，input word 和 output word 都会被我们进行 one-hot 编码。仔细想一下，我们的输入被 one-hot 编码以后大多数维度上都是0（实际上仅有一个位置为1），所以这个向量相当稀疏，那么会造成什么结果呢。如果我们应用&lt;strong&gt;矩阵乘法&lt;/strong&gt;将一个1 x 10000的向量和10000 x 300的矩阵相乘，它会消耗相当大的计算资源，为了高效计算，它仅仅会选择矩阵中对应的向量中维度值为1的索引行，看图就明白。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cs224n/matrix_mult_w_one_hot.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;为了有效地进行计算，这种稀疏状态下不会进行矩阵乘法计算，可以看到计算的结果实际上是矩阵的某一行：先根据 input word 的 one-hot 向量中元素 1 的索引，再由这个索引取得矩阵对应的行。上面的例子中，左边向量中取值为 1 的对应维度为 3（下标从0开始），那么计算结果就是矩阵的第 3 行（下标从0开始）—— [10, 12, 19]，这样模型中的隐层权重矩阵便成了一个”查找表“（lookup table），进行矩阵计算时，直接去查输入向量中取值为1的维度下对应的那些权重值。隐层的输出就是每个输入单词的“嵌入词向量”，也称为输入词向量。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;上面提到的隐层权重矩阵是从输入层到隐藏层的权重矩阵，称为&lt;strong&gt;输入词向量矩阵&lt;/strong&gt;，在接下来的章节中我们还会看到，从隐藏层到输出层的权重矩阵，称为&lt;strong&gt;输出词向量矩阵&lt;/strong&gt;。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;3  输出层&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;经过神经网络隐层的计算，“ants“ 这个词会从一个 1 x 10000 的向量变成 1 x 300 的向量，再被输入到输出层。输出层是一个softmax回归分类器，它的每个结点将会输出一个 0-1 之间的值（概率），这些所有输出层神经元结点的概率之和为1。&lt;/p&gt;

&lt;p&gt;下面是一个例子，训练样本为 (input word: “ants”， output word: “car”) 的计算示意图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/in-post/post-cs224n/output_weights_function.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;乘号右侧的 output weights for “car” 即是 “car” 的输出词向量，“car”在作为 input word 时也有相应的输入词向量。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;直觉上的理解，如果两个不同的单词有着非常相似的“上下文”（也就是窗口单词很相似，比如“Kitty climbed the tree”和“Cat climbed the tree”），那么通过我们的模型训练，这两个单词的嵌入向量将非常相似。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;在word2vec 中，规定两个单词的&lt;strong&gt;相似度&lt;/strong&gt;就是各自词向量的内积，如，(input word: “ants”， output word: “car”) 的计算结果就是“ants”和“car”的相似度。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;那么两个单词拥有相似的“上下文”到底是什么含义呢？比如对于同义词“intelligent”和“smart”，我们觉得这两个单词应该拥有相同的“上下文”。而例如”engine“和”transmission“这样相关的词语，可能也拥有着相似的上下文。&lt;/p&gt;

&lt;p&gt;实际上，这种方法实际上也可以帮助你进行词干化（stemming），例如，神经网络对”ant“和”ants”两个单词会习得相似的词向量。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;词干化（stemming）就是去除词缀得到词根的过程。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;以上便是 word2vec 模型的最简化形式，CBOW 和 Skip-Gram 无非就是对这个模型的推广，到这里，大家应该在直观上理解了这个模型，如若要深究下去（数学推导👀），我会在下一篇文章中具体分析。&lt;/p&gt;

&lt;h1 id=&quot;4-小结&quot;&gt;4 小结&lt;/h1&gt;

&lt;p&gt;这篇文章从自然语言开始讲起，通过如何表示一个词，讲述了从词的离散表示到稠密表示的发展过程，引入了共现矩阵、词向量、SVD、word2vec 等方法将一个具体的单词（符号）表示成可以喂给任意模型处理的数据。在下一篇文章中，我将介绍 word2vec 的两种具体的模型：CBOW 和 Skip-Gram，以及它们各自的目标函数梯度的计算和梯度下降算法，还有它们的训练方法：负采样 (Negative sampling)和 Hierarchical softmax。&lt;/p&gt;
</description>
        <pubDate>Fri, 16 Aug 2019 04:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/08/16/Stanford-CS224n-nlp-02/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/08/16/Stanford-CS224n-nlp-02/</guid>
        
        <category>NLP</category>
        
        <category>Stanford</category>
        
        <category>深度学习</category>
        
        
      </item>
    
      <item>
        <title>Stanford CS224n 自然语言处理（一）</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;Begining&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;

&lt;p&gt;我是在4月初入坑自然语言处理的，并在4月初到6月初的2个月时间里面学习了斯坦福的自然语言处理课程。接下来我会大致介绍一下这门课程，讲讲每节课的大纲，以及我觉得缺少的东西。&lt;/p&gt;

&lt;p&gt;首先给出课程的&lt;a href=&quot;http://web.stanford.edu/class/cs224n/&quot;&gt;官方网站&lt;/a&gt;，所有的课程资料都可以在这个网页上下载，包括：课程ppt、作业说明文档、手册、部分笔记等。配套的网络课程在&lt;a href=&quot;https://www.bilibili.com/video/av46065585?from=search&amp;amp;seid=12274272452479951197&quot;&gt;这里&lt;/a&gt;，如果你希望系统地学习 NLP，那么这门课程是你的首选。(6月下旬斯坦福又开放了一门课程 CS224u 自然语言理解，我会另开一个系列介绍)&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;课程大纲&quot;&gt;课程大纲&lt;/h2&gt;

&lt;h4 id=&quot;lecture-1--introduction-and-word-vectors&quot;&gt;Lecture 1 : Introduction and Word Vectors&lt;/h4&gt;

&lt;p&gt;首先介绍了课程的教授者，课程资料以及作业的评分细则，然后列出了和2017年的 CS2224n 课程的不同点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;课程涵盖新的模型和方法，如，字符级模型( Character Model )、Transformer、多任务学习( Multitask Learning )&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;作业涵盖了新内容，如，NMT with attention、卷积网络( ConvNets )、子词模型( subword model )&lt;/li&gt;
  &lt;li&gt;使用 PyTorch 而不是 TensorFlow&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;接着介绍了自然语言以及词义；词向量、word2vec 概览；word2vec 的目标函数梯度的计算以及梯度下降算法。&lt;/p&gt;

&lt;h4 id=&quot;lecture-2--word-vectors-and-word-senses&quot;&gt;Lecture 2 : Word Vectors and Word Senses&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;实现 word2vec 的两种模型
    &lt;ul&gt;
      &lt;li&gt;Skip-grams (SG)&lt;/li&gt;
      &lt;li&gt;Continuous Bag of Words (CBOW)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;训练技巧
    &lt;ul&gt;
      &lt;li&gt;负采样 (Negative sampling)&lt;/li&gt;
      &lt;li&gt;Hierarchical softmax&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;共现矩阵( co-occurrence matrix )，以及降维方法( SVD )&lt;/li&gt;
  &lt;li&gt;GloVe&lt;/li&gt;
  &lt;li&gt;词向量的评价指标&lt;/li&gt;
  &lt;li&gt;词义：歧义( word sense ambiguity )、一词多义( polysemy )等&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-3--word-window-classification-neural-networks-and-matrix-calculus&quot;&gt;Lecture 3 : Word Window Classification, Neural Networks and Matrix Calculus&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;分类问题概览&lt;/li&gt;
  &lt;li&gt;神经网络简介&lt;/li&gt;
  &lt;li&gt;命名实体识别( Named Entity Recognition )&lt;/li&gt;
  &lt;li&gt;词窗口&lt;/li&gt;
  &lt;li&gt;矩阵求导&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-4--backpropagation-and-computation-graphs&quot;&gt;Lecture 4 : Backpropagation and computation graphs&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;推导矩阵反向传播的梯度&lt;/li&gt;
  &lt;li&gt;计算图( computation graphs )&lt;/li&gt;
  &lt;li&gt;神经网络的一些需要知道的问题：
    &lt;ul&gt;
      &lt;li&gt;向量化表达( Vectorization ) / 矩阵化表达( Matrixization )的计算速度优势&lt;/li&gt;
      &lt;li&gt;过拟合( overfitting )&lt;/li&gt;
      &lt;li&gt;非线性( nonlinearities )&lt;/li&gt;
      &lt;li&gt;初始化( initialization )&lt;/li&gt;
      &lt;li&gt;优化器( optimizers )&lt;/li&gt;
      &lt;li&gt;学习率( learning rates )&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-5--dependency-parsing&quot;&gt;Lecture 5 : Dependency Parsing&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;两种句法结构：
    &lt;ul&gt;
      &lt;li&gt;一致性( Consistency ) ，即短语结构语法，也即无上下文语法( context-free grammars )，18课会单独讲&lt;/li&gt;
      &lt;li&gt;依存性( Dependency )，即依存结构&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;依存语法和依存结构&lt;/li&gt;
  &lt;li&gt;依存关系解析( Dependency Parsing )
    &lt;ul&gt;
      &lt;li&gt;基于转换的解析( transition-based )&lt;/li&gt;
      &lt;li&gt;基于神经网络的解析( nueral )&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-6--language-models-and-recurrent-neural-networks&quot;&gt;Lecture 6 : Language Models and Recurrent Neural Networks&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;语言模型简介&lt;/li&gt;
  &lt;li&gt;n-gram 语言模型&lt;/li&gt;
  &lt;li&gt;神经网络（RNN）语言模型&lt;/li&gt;
  &lt;li&gt;评估语言模型&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-7--vanishing-gradients-and-fancy-rnns&quot;&gt;Lecture 7 : Vanishing Gradients and Fancy RNNs&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;梯度消失问题和梯度爆炸问题及其解决方法&lt;/li&gt;
  &lt;li&gt;LSTM （ Long Short-Term Memory ）&lt;/li&gt;
  &lt;li&gt;GRU（ Gated Recurrent Unit ）&lt;/li&gt;
  &lt;li&gt;RNN variant：
    &lt;ul&gt;
      &lt;li&gt;Bidirectional RNN&lt;/li&gt;
      &lt;li&gt;Multi-layer RNN&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-8--machine-translation-sequence-to-sequence-and-attention&quot;&gt;Lecture 8 : Machine Translation, Sequence-to-sequence and Attention&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;统计机器翻译（ SMT ）
    &lt;ul&gt;
      &lt;li&gt;对齐（alignment）&lt;/li&gt;
      &lt;li&gt;解码（decodeing）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;神经机器翻译（ NMT ）
    &lt;ul&gt;
      &lt;li&gt;seq2seq&lt;/li&gt;
      &lt;li&gt;beam search decoding&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Attention 机制&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-9--practical-tips-for-final-projects&quot;&gt;Lecture 9 : Practical Tips for Final Projects&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;最终项目的一些建议&lt;/li&gt;
  &lt;li&gt;回顾 GRU、LSTM&lt;/li&gt;
  &lt;li&gt;机器翻译的评价指标：BLEU&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-10--textual-question-answering&quot;&gt;Lecture 10 : (Textual) Question Answering&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;QA —&amp;gt;机器阅读理解&lt;/li&gt;
  &lt;li&gt;SQuAD 数据集&lt;/li&gt;
  &lt;li&gt;斯坦福 Attentive Reader&lt;/li&gt;
  &lt;li&gt;BiDAF&lt;/li&gt;
  &lt;li&gt;一些先进的模型：Co-attention、FusionNet 等&lt;/li&gt;
  &lt;li&gt;ELMo 和 BERT 概览&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-11--convnets-for-nlp&quot;&gt;Lecture 11 : ConvNets for NLP&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;从 RNN 到 CNN&lt;/li&gt;
  &lt;li&gt;模型比较&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-12--information-from-parts-of-words--subword-models&quot;&gt;Lecture 12 : Information from parts of words : Subword Models&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;语言学概览
    &lt;ul&gt;
      &lt;li&gt;发音（Phonetics）&lt;/li&gt;
      &lt;li&gt;音韵（Phonology）&lt;/li&gt;
      &lt;li&gt;词的形态（Morphology）&lt;/li&gt;
      &lt;li&gt;书写系统&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;字符级的模型&lt;/li&gt;
  &lt;li&gt;子词模型
    &lt;ul&gt;
      &lt;li&gt;Byte Pair Encoding&lt;/li&gt;
      &lt;li&gt;Wordpiece&lt;/li&gt;
      &lt;li&gt;Sentencepiece&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;混合神经网络翻译&lt;/li&gt;
  &lt;li&gt;fastText&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-13--contextual-word-representations-and-pretraining&quot;&gt;Lecture 13 : Contextual Word Representations and Pretraining&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;词表示&lt;/li&gt;
  &lt;li&gt;pre-ELMo &amp;amp; EMLo&lt;/li&gt;
  &lt;li&gt;ULMfit&lt;/li&gt;
  &lt;li&gt;Transformer 架构
    &lt;ul&gt;
      &lt;li&gt;self-attention&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;BERT&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-14--self-attention-for-generative-models&quot;&gt;Lecture 14 : Self-Attention for Generative Models&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;Self-Attention&lt;/li&gt;
  &lt;li&gt;文本生成&lt;/li&gt;
  &lt;li&gt;图像生成&lt;/li&gt;
  &lt;li&gt;音乐生成&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-15--natural-language-generation&quot;&gt;Lecture 15 : Natural Language Generation&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;回顾语言模型和解码算法及技巧
    &lt;ul&gt;
      &lt;li&gt;Greedy decoding&lt;/li&gt;
      &lt;li&gt;Beam search&lt;/li&gt;
      &lt;li&gt;Sampling methods&lt;/li&gt;
      &lt;li&gt;Softmax temperature&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;自然语言生成（NLG）任务以及神经网络方法
    &lt;ul&gt;
      &lt;li&gt;文本总结&lt;/li&gt;
      &lt;li&gt;对话系统&lt;/li&gt;
      &lt;li&gt;创意写作&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;NLG 的评估&lt;/li&gt;
  &lt;li&gt;NLG 的趋势和发展&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-16--coreference-resolution&quot;&gt;Lecture 16 : Coreference Resolution&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;共指解析及其应用&lt;/li&gt;
  &lt;li&gt;提及检测（Mention Detection）&lt;/li&gt;
  &lt;li&gt;一些语言学概念
    &lt;ul&gt;
      &lt;li&gt;anaphora&lt;/li&gt;
      &lt;li&gt;cataphora&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;共指模型
    &lt;ul&gt;
      &lt;li&gt;Rule-based Model&lt;/li&gt;
      &lt;li&gt;Mention Pair Model&lt;/li&gt;
      &lt;li&gt;Mention Ranking Model&lt;/li&gt;
      &lt;li&gt;Mention Clustering Model&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;评估指标&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-17--multitask-learning-as-qa&quot;&gt;Lecture 17 : Multitask Learning as QA&lt;/h4&gt;

&lt;p&gt;这节课相当于一个讲座，介绍了多任务学习的若干模型、训练策略，以及多任务学习的优势&lt;/p&gt;

&lt;h4 id=&quot;lecture-18--tree-recursive-neural-networks-constituency-parsing-and-sentiment&quot;&gt;Lecture 18 : Tree Recursive Neural Networks, Constituency Parsing, and Sentiment&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;递归神经网络（Recursive Neural Networks）&lt;/li&gt;
  &lt;li&gt;短语句法解析&lt;/li&gt;
  &lt;li&gt;树递归神经网络（TreeRNN）&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;lecture-19--bias-in-the-vision-and-language-of-artificial-intelligence&quot;&gt;Lecture 19 : Bias in the Vision and Language of Artificial Intelligence&lt;/h4&gt;

&lt;p&gt;讲座，介绍了 AI 的偏见以及多任务学习&lt;/p&gt;

&lt;h4 id=&quot;lecture-20--the-future-of-deep-learning--nlp&quot;&gt;Lecture 20 : The Future of Deep Learning + NLP&lt;/h4&gt;

&lt;p&gt;讲座，介绍了大规模深度学习在各个领域取得的成就，无标签数据集，无监督机器翻译，大模型（如 BERT、GPT-2），多任务学习，更加难的自然语言理解，NLP 在工业上的应用，以及未来的挑战。&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;后记&quot;&gt;后记&lt;/h2&gt;

&lt;p&gt;以上就是Stanford CS224n 自然语言处理课程的课程概览，我会在今后陆续完成每节课的分析与笔记（如果有时间的话 hhh）。&lt;/p&gt;

&lt;p&gt;自然语言处理真的是一门非常庞大的学科，其中包含了很多内容，想要在一门课上学习完所有的内容是不现实的，个人所见，这门课的的意义就在于为刚开始学习自然语言处理的学生做一个相对详细的 review ，接下去要做什么方向，按照自己的兴趣来。&lt;/p&gt;

&lt;p&gt;在前言中我提到过，这门课程还有许多没有提及的方面，比如分词方法、TF-IDF、关系提取、情感分析等，如果对这些感兴趣的话，Stanford 为大家又提供了一门课程（&lt;a href=&quot;http://web.stanford.edu/class/cs224u/&quot;&gt;CS224u 自然语言理解&lt;/a&gt;），感谢斯坦福。&lt;/p&gt;

&lt;p&gt;写这些文章的初衷是为了对抗遗忘，所以某种程度上是给自己温故用的。因此，我不会把课上的内容完完整整地记录下来，如果有什么疑问，欢迎指出。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PS&lt;/strong&gt; ：博客的评论区还没有调试成功，如果我的文章有什么错误，烦请点击下面的微博按钮，私信给我，不胜感激。&lt;/p&gt;

&lt;hr /&gt;

</description>
        <pubDate>Sun, 04 Aug 2019 23:04:00 +0800</pubDate>
        <link>http://localhost:4000/2019/08/04/Stanford-CS224n-nlp-01/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/08/04/Stanford-CS224n-nlp-01/</guid>
        
        <category>NLP</category>
        
        <category>Stanford</category>
        
        <category>深度学习</category>
        
        
      </item>
    
      <item>
        <title>Hello World</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;“Yeah It’s on.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;

&lt;p&gt;我的Blog就这么开通了。&lt;/p&gt;

&lt;p&gt;鼓捣了两天，不容易啊。。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#build&quot;&gt;这里推荐几个写的比较好的教程&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;其实很早就想建一个小站，记录一下学习和生活，因为自己记性不好，总想着留下点印记，以便回溯，那么就从今日开始吧。&lt;/p&gt;

&lt;p id=&quot;build&quot;&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;a href=&quot;http://bluebiu.com/blog/create-blog-in-github.html&quot;&gt;从零开始用GitHub Pages创建博客&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;[http://bluebiu.com/blog/learn-to-use-jekyll.html#12%E7%AE%80%E5%8D%95%E5%A5%97%E7%94%A8](http://bluebiu.com/blog/learn-to-use-jekyll.html#12简单套用)&quot;&gt;从零开始折腾Jekyll&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://ambeta.github.io/2016/03/27/build-website-with-jekyll.html#jekyll-template&quot;&gt;如何使用jekyll建站&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://chaosinmotion.coding.me/cblog/2016/03/26/build-a-blog/&quot;&gt;Github Pages + Jekyll 独立博客一小时快速搭建&amp;amp;上线指南&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;正文&quot;&gt;正文&lt;/h2&gt;

&lt;p&gt;接下来说说搭建这个博客的过程。&lt;/p&gt;

&lt;p&gt;我的操作系统是macOS，一开始装了好多依赖，gem、ruby、jekyll 等等。我觉得每个人的情况不同，我按照教程指引，在配置的过程中同样遇到了教程上所没有提及的问题，但总的来说上面我推荐的教程是比较易于操作的，如若遇到了玄学问题，请自行 Google + baidu ，基本都能解决。&lt;/p&gt;

&lt;p&gt;我是按照&lt;a href=&quot;https://pages.github.com/&quot;&gt;GitHub Pages&lt;/a&gt; + &lt;a href=&quot;http://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt; 快速 Building Blog 的技术方案，非常轻松时尚。&lt;/p&gt;

&lt;p&gt;其优点非常明显：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Markdown&lt;/strong&gt; 带来的优雅写作体验&lt;/li&gt;
  &lt;li&gt;非常熟悉的 Git workflow ，&lt;strong&gt;Git Commit 即 Blog Post&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;利用 GitHub Pages 的域名和免费无限空间，不用自己折腾主机
    &lt;ul&gt;
      &lt;li&gt;如果需要自定义域名，也只需要简单改改 DNS 加个 CNAME 就好了&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Jekyll 的自定制非常容易，基本就是个模版引擎&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我的 jekyll 模板直接 fork 了 &lt;a href=&quot;https://github.com/Huxpro/huxpro.github.io&quot;&gt;Hux Blog&lt;/a&gt;（为大佬喝彩。）&lt;/p&gt;

&lt;p&gt;请按照教程一步一步设置，过程可能会有些繁琐，但小站做好之后幸福感油然而生。&lt;/p&gt;

&lt;h2 id=&quot;后记&quot;&gt;后记&lt;/h2&gt;

&lt;p&gt;如果你恰巧逛到了我的博客，希望你能喜欢。&lt;/p&gt;

&lt;p&gt;—— chenjing 记于2019.8&lt;/p&gt;
</description>
        <pubDate>Sat, 03 Aug 2019 22:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/08/03/Hello-World/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/08/03/Hello-World/</guid>
        
        <category>生活</category>
        
        
      </item>
    
  </channel>
</rss>
