<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">
    <meta name="description" content="记录 | 陈晶，Web & Mobile Lover，Algrithm Engineer | 这里是 @chenjing 的个人博客，与你一起发现更大的世界。">
    <meta name="keywords"  content="陈晶, @Kgoeson, Hux Blog, 博客, 个人网站, 互联网, 机器学习, 深度学习, 自然语言处理">
    <meta name="theme-color" content="#000000">
    
    <!-- Open Graph -->
    <meta property="og:title" content="Stanford CS224n 自然语言处理（三） - 陈晶的博客 | CJ Blog">
    
    <meta property="og:type" content="article">
    <meta property="og:description" content="0 前言

">
    
    <meta property="article:published_time" content="2019-11-05T20:00:00Z">
    
    
    <meta property="article:author" content="chenjing">
    
    
    <meta property="article:tag" content="NLP">
    
    <meta property="article:tag" content="Stanford">
    
    <meta property="article:tag" content="深度学习">
    
    
    <meta property="og:image" content="http://localhost:4000/img/cj-selfie.jpeg">
    <meta property="og:url" content="http://localhost:4000/2019/11/05/Stanford-CS224n-nlp-03/">
    <meta property="og:site_name" content="陈晶的博客 | CJ Blog">
    
    <title>Stanford CS224n 自然语言处理（三） - 陈晶的博客 | CJ Blog</title>

    <!-- Web App Manifest -->
    <link rel="manifest" href="/pwa/manifest.json">

    <!-- Favicon -->
    <link rel="shortcut icon" href="/img/favicon.ico">
    
    <!-- Canonical URL -->
    <link rel="canonical" href="http://localhost:4000/2019/11/05/Stanford-CS224n-nlp-03/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/hux-blog.min.css">

    <!-- Custom Fonts -->
    <!-- <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- Navigation -->

<nav class="navbar navbar-default navbar-custom navbar-fixed-top">

    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">ChenJing's Blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>
                    
                    
                    
                    
                    <li>
                        <a href="/about/">About</a>
                    </li>
                    
                    
                    
                    <li>
                        <a href="/archive/">Archive</a>
                    </li>
                    
                    
                    
                    
                    
                    <li>
                        <a href="/portfolio/">Portfolio</a>
                    </li>
                    
                    
                    
                    
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    var __HuxNav__ = {
        close: function(){
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        },
        open: function(){
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }

    // Bind Event
    $toggle.addEventListener('click', function(e){
        if ($navbar.className.indexOf('in') > 0) {
            __HuxNav__.close()
        }else{
            __HuxNav__.open()
        }
    })

    /**
     * Since Fastclick is used to delegate 'touchstart' globally
     * to hack 300ms delay in iOS by performing a fake 'click',
     * Using 'e.stopPropagation' to stop 'touchstart' event from 
     * $toggle/$collapse will break global delegation.
     * 
     * Instead, we use a 'e.target' filter to prevent handler
     * added to document close HuxNav.  
     *
     * Also, we use 'click' instead of 'touchstart' as compromise
     */
    document.addEventListener('click', function(e){
        if(e.target == $toggle) return;
        if(e.target.className == 'icon-bar') return;
        __HuxNav__.close();
    })
</script>


    <!-- Image to hack wechat -->
<!-- <img src="/img/icon_wechat.png" width="0" height="0"> -->
<!-- <img src="/img/post-bg-unix-linux.jpg" width="0" height="0"> -->

<!-- Post Header -->



<style type="text/css">
    header.intro-header{
        position: relative;
        background-image: url('/img/post-bg-unix-linux.jpg');
        background: ;
    }

    
</style>

<header class="intro-header" >

    <div class="header-mask"></div>
    
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/archive/?tag=NLP" title="NLP">NLP</a>
                        
                        <a class="tag" href="/archive/?tag=Stanford" title="Stanford">Stanford</a>
                        
                        <a class="tag" href="/archive/?tag=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" title="深度学习">深度学习</a>
                        
                    </div>
                    <h1>Stanford CS224n 自然语言处理（三）</h1>
                    
                    <h2 class="subheading"></h2>
                    <span class="meta">Posted by chenjing on November 5, 2019</span>
                </div>
            </div>
        </div>
    </div>
</header>






<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

    <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <!-- Multi-Lingual -->
                

				<h1 id="0-前言">0 前言</h1>

<p>这篇文章涵盖 CS224n Lecture 2 中的部分内容。主要内容包括：word2vec 的目标函数梯度的计算；实现 word2vec 的两种模型：Skip-Gram 和 Continuous Bag-of-Word；训练方法：负采样 (Negative sampling)和 Hierarchical softmax。</p>

<p>我们回顾一下 word2vec，word2vec 模型实际上包含：</p>

<ul>
  <li><strong>两个算法（模型）</strong>：continuous bag-of-words（CBOW）和 skip-gram。CBOW 是根据上下文单词来预测中心词。skip-gram 则相反，是根据中心词预测上下文词。</li>
  <li><strong>两种训练方法：</strong>negative sampling 和 hierarchical softmax。Negative sampling 通过抽取负样本来定义目标，hierarchical softmax 通过使用一个有效的树结构来计算所有词的概率来定义目标。</li>
</ul>

<h1 id="1-continuous-bag-of-word-model">1 Continuous Bag-of-Word Model</h1>

<h2 id="11-one-word-context">1.1 One-word context</h2>

<p>同样，我们首先从简单的例子开始。其实，one-word context 就是 3.2 中任务实施的示例，在这一节，我们不再举具体的示例，而是通过<strong>数学公式</strong>来表征这一过程。我们规定，如若没有特殊说明，向量皆指<strong>列向量</strong>。</p>

<table>
  <tbody>
    <tr>
      <td>⚠️警告，接下来会出现大篇幅的数学公式推导，若有看不明白的地方，请自己举一些具体的例子，或者参考 Stanford CS224n 自然语言处理（二） 3.2 中的任务实施示例</td>
    </tr>
  </tbody>
</table>

<p><img src="/img/in-post/post-cs224n/one_word_net_arch_1.png" alt="" /></p>

<p>上图是 CBOW 的最简化模型（实际上也是 Skip-Gram 的最简化模型），输入只有一个上下文词，且只预测一个目标词（即中心词）。 $\mathbf{W} \in \mathbb{R}^{V \times N}$ 是输入层到隐藏层的权重矩阵，$\mathbf{W}^{\prime} \in \mathbb{R}^{N \times V}$ 是隐藏层到输出层的权重矩阵，$\mathbf{x}$ 是输入 one-hot 向量，$\mathbf{h}$ 是隐藏向量，$\mathbf{y}$ 是输出概率向量， ${x_{1}, \cdots, x_{V}}$ ， ${h_{1}, \cdots, h_{N}}$ ， ${y_{1}, \cdots, y_{V}}$ ，分别代表输入层、隐藏层、输出层的神经元（units），其中 ${x_{1}, \cdots, x_{V}}$ ，中只有一个值为 1，其余值都为 0，我们假设 $x_{k}=1$，$x_{k^{\prime}}=0$， ${k^{\prime} \neq k}$。 $\mathbf{v}_w^{\mathrm{T}}$ 为 $\mathbf{W}^{\mathrm{T}}$ 的某一列，也就是说，$\mathbf{v}_w$ 为 $\mathbf{W}$ 的某一行，这里行列转换最好画个简单的矩阵辅助理解。我们有：</p>

<script type="math/tex; mode=display">\mathbf{h}=\mathbf{W}^{\mathrm{T}} \mathbf{x}=\mathbf{W}_{(k, \cdot)}^{ \quad \mathrm{T}} :=\mathbf{v}_{w_{I}}^{\mathrm{T}}  \tag{1}</script>

<p>其中$\mathbf{v}<em>{w</em>{I}}^{\mathrm{T}}$是个列向量，为$\mathbf{W}^{\mathrm{T}}$第 $k$ 列 ，形式上等于 $\mathbf{W}$ 的第 $k$ 行，这里的 $\mathbf{W}<em>{(k, \cdot)}^{\quad \mathrm{\mathrm{T}}}$ 的意思是先取 $\mathbf{W}$ 的第 $k$ 行，再转置 ，同样地， $\mathbf{h}$ 是个列向量，为 $\mathbf{W}^{\mathrm{T}}$ 第 $k$ 列 ，形式上等于 $\mathbf{W}$ 的第 $k$ 行。 $\mathbf{v}</em>{w_{I}}$ 是输入词 ${w_{I}}$ 的向量表示，即词向量。这样解释不知大家明白否。</p>

<p>其实这里暗示了从输入层隐藏层的激活函数是线性的，因为从 $\mathbf{v}<em>{w</em>{I}}$ 到 $\mathbf{h}$ 只经过一个转置。</p>

<p>从隐藏层到输出层有一个不同的权重矩阵 $\mathbf{W}^{\prime} \in \mathbb{R}^{N \times V}$ ，通过这个矩阵，对词表中的每一个词，我们最终能计算得到一个分数 $u_{j}，j = 1,2,\cdots,V$。</p>

<script type="math/tex; mode=display">u_{j}=\mathbf{v}_{w_{j}}^{\prime \mathrm{T}} \mathbf{h}  \tag{2}</script>

<p>（2）中，$\mathbf{v}<em>{w</em>{j}}^{\prime}$ 是 $\mathbf{W}^{\prime}$ 的第 $j$ 列，然后我们使用 softmax 函数将分数转化成后验概率。</p>

<script type="math/tex; mode=display">p\left(w_{j} \mid w_{I}\right)=y_{j}= softmax(u_j)=\dfrac{\exp \left(u_{j}\right)}{\sum_{j^{\prime}=1}^{V} \exp \left(u_{j^{\prime}}\right)}  \tag{3}</script>

<p>（3）中，$y_{j}$ 是输出层第 $j$ 个单元的输出，将（1）（2）带入（3）得到：</p>

<script type="math/tex; mode=display">p\left(w_{j} \mid w_{I}\right)=\dfrac{\exp \left(\mathbf{v}_{w_{j}}^{\prime \ \mathrm{T}} \mathbf{v}_{w_{I}}\right)}{\sum_{j^{\prime}=1}^{V} \exp \left(\mathbf{v}_{w_{j^{\prime}}}^{\prime  \ {\mathrm{T}}} {\mathbf{v}_{w_{I}}}\right)}  \tag{4}</script>

<p>通过以上几个式子，我们发现，似乎 $\mathbf{v}_w^{\prime}$ 也可以作为词向量，因为输出的理想状态也是一个 one-hot 向量。因此，$\mathbf{v}_w$ 与 $\mathbf{v}_w^{\prime}$ 是同一个词 $w$ 的两种词向量表示（前者称为<strong>输入词向量</strong>，后者称为<strong>输出词向量</strong>）。又因为 $\mathbf{v}_w$ 来自于 $\mathbf{W}$ 的行，$\mathbf{v}_w^{\prime}$ 来自于 $\mathbf{W}^{\prime}$ 的列，所以，我们称 $\mathbf{W}$ 为<strong>输入词向量矩阵</strong>，$\mathbf{W}^{\prime}$ 为<strong>输出词向量矩阵</strong>。</p>

<blockquote>
  <p>在之后的论述中，我们将 $\mathbf{v}<em>w$ 与 $\mathbf{v}</em>{w}^{\mathrm{T}}$ 都称为输入词向量，只不过前者是行向量，后者是列向量。</p>

  <p>将 $\mathbf{v}_w^{\prime}$ 与 $\mathbf{v}_w^{\prime \; \mathrm{T}}$ 都称为输出词向量，只不过前者是列向量，后者是行向量。</p>
</blockquote>

<h3 id="111-更新-mathbfwprime">1.1.1 更新 $\mathbf{W}^{\prime}$</h3>

<p>接下来，我们推导该模型权重更新的式子。 虽然实际中应用这样的式子来更新权重不具备可操作性（之后会解释），但这样的推导有利于我们深入理解原始模型而不应用任何技巧。</p>

<p>对于一个训练样本，我们的训练目标是最大化（4）​，即</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} \max p\left(w_{O} \mid w_{I}\right) \tag{5} &=\max y_{j^{*}} \\ &=\max \log y_{j^{*}} \tag{6} \\ &=u_{j^{*}}-\log \sum_{j^{\prime}=1}^{V} \exp \left(u_{j^{\prime}}\right) :=-E \tag{7} \end{align} %]]></script>

<p>$w_{O}$ 为实际的输出词（output word），我们记 $j^<em>$ 为 $w_{O}$ 在输出层的索引。 $E=-\log p\left(w_{O} \mid w_{I}\right)=\log \sum_{j^{\prime}=1}^{V} \exp \left(u_{j^{\prime}}\right)-u_{j^{</em>}}$ 是我们的损失函数，和（7）相差一个负号，所以我们需要最小化 $E$ ， 这个损失函数是交叉熵损失的一种特殊情况，单个样本的二元交叉熵损失如下：</p>

<script type="math/tex; mode=display">L=-[y \log \hat{y}+(1-y) \log (1-\hat{y})]</script>

<p>$y$ 是真实标签，$\hat{y}$ 是预测值，当 $y=1$ 时，上式变为 $L=-\log \hat{y}$ ，形式上和我们的损失函数一致。</p>

<p>现在我们来推导从隐藏层到输出层的权重更新公式。我们从损失函数出发，求 $E$ 关于 $u_{j}$ （从隐藏层输入到输出层的第 $j$ 个单元的分数）的偏导数，</p>

<script type="math/tex; mode=display">\dfrac{\partial E}{\partial u_{j}}=y_{j}-t_{j} :=e_{j} \tag{8}</script>

<p>其中，$t_{j}=\mathbf{1}\left(j=j^{*}\right)$ ，意思是 $t_{j}$ 只有在第 $j$ 个单元为实际的输出词时才等于1，否则为0，也就是说 $\mathbf{t}=(t_{1} \; t_{2}  \cdots t_{V})^{\mathrm{T}}$ 是一个 one-hot  向量。注意，（8）仅仅是输出层的预测误差。</p>

<p>然后，求求 $E$ 关于 $w_{ij}^{\prime}$ 的偏导数，也就得到了从隐藏层到输出层的权重更新的梯度。</p>

<script type="math/tex; mode=display">\dfrac{\partial E}{\partial w_{i j}^{\prime}}=\dfrac{\partial E}{\partial u_{j}} \cdot \dfrac{\partial u_{j}}{\partial w_{i j}^{\prime}}=e_{j} \cdot h_{i} \tag{9}</script>

<p>所以，当使用随机梯度下降（stochastic gradient descent）时，我们得到更新公式：</p>

<script type="math/tex; mode=display">w_{i j}^{\prime \; (\text{new})}=w_{i j}^{\prime \; (\text{old})}-\eta \cdot e_{j} \cdot h_{i} \tag{10}</script>

<p>或者，</p>

<script type="math/tex; mode=display">\mathbf{v}_{w_{j}}^{\prime \; (\text{new})}=\mathbf{v}_{w_{j}}^{\prime \; (\text{old})}-\eta \cdot e_{j} \cdot \mathbf{h} \quad \text { for } j=1,2, \cdots, V \tag{11}</script>

<p>其中，$\eta&gt;0$ 是学习率，$e_{j}=y_{j}-t_{j}$ ， $h_{i}$ 是隐藏层的第 $i$ 个单元， $\mathbf{v}<em>{w</em>{j}}^{\prime }$ 是 $w_j$ 的输出词向量。这个更新公式意味着我们需要遍历整个词汇表，检查每个单词的输出概率 $y_j$ ，并与我们期望的输出 $t_j$ 相比较。</p>

<p>从（11）中还可以看出，如果 $y_j&gt;t_j$ ，我们就从 $\mathbf{v}<em>{w</em>{j}}^{\prime }$ 中减去隐藏向量 $\mathbf{h}$ （在这里，指的就是 $\mathbf{v}<em>{w</em>{I}}$）的一部分，使得 $\mathbf{v}<em>{w</em>{j}}^{\prime }$ 远离 $\mathbf{v}<em>{w</em>{I}}$。如果 $y_j&lt;t_j$ （只会在 $t_{j}=1$ 时发生，即 $w_{j}=w_{O}$） ，我们就给 $\mathbf{v}<em>{w</em>{j}}^{\prime }$ 中加上隐藏向量 $\mathbf{h}$ 的一部分，使得 $\mathbf{v}<em>{w</em>{O}}^{\prime }$ 接近 $\mathbf{v}<em>{w</em>{I}}$。如果 $y_j$ 与 $t_j$ 非常接近，那么基本不更新，一次迭代训练完毕。</p>

<blockquote>
  <p>这里的“接近”和“远离”，指的是两个词向量的内积越大越接近，内积越小越远离。其实很好理解，两个相同的词向量的内积最大，此时，这两个词向量完全相同，也就是最“接近”。</p>
</blockquote>

<h3 id="112-更新-mathbfw">1.1.2 更新 $\mathbf{W}$</h3>

<p>我们得到了 $\mathbf{W}^{\prime}$ 更新式之后，现在的目标就是求得 $\mathbf{W}$ 的更新式。现在，我们求 $E$ 关于 $h_{i}$ 的偏导数，</p>

<script type="math/tex; mode=display">\dfrac{\partial E}{\partial h_{i}}=\sum_{j=1}^{V} \dfrac{\partial E}{\partial u_{j}} \cdot \dfrac{\partial u_{j}}{\partial h_{i}}=\sum_{j=1}^{V} e_{j} \cdot w_{i j}^{\prime} :=\mathrm{EH}_{i} \tag{12}</script>

<p>其中，$h_{i}$ 是隐藏层的第 $i$ 个单元， $u_{j}$、 $e_{j}$ 与 1.1.1 中的定义一致， $EH$ 是一个 $n$ 维向量。</p>

<p>我们回顾一下隐藏层做的线性计算操作，根据（1），我们可以知道，</p>

<script type="math/tex; mode=display">h_{i}=\sum_{k=1}^{V} x_{k} \cdot w_{k i} \tag{13}</script>

<p>然后我们求 $E$ 关于 $\mathbf{W}$ 每一个元素 $w_{kj}$ 的偏导数，</p>

<script type="math/tex; mode=display">\dfrac{\partial E}{\partial w_{k i}}=\dfrac{\partial E}{\partial h_{i}} \cdot \dfrac{\partial h_{i}}{\partial w_{k i}}=\mathrm{EH}_{i} \cdot x_{k} \tag{14}</script>

<p>所以我们得到，</p>

<script type="math/tex; mode=display">\dfrac{\partial E}{\partial \mathbf{W}}=\mathbf{x} \otimes \mathbf{E H}=\mathbf{x} \mathrm{EH}^{\mathrm{T}} \tag{15}</script>

<blockquote>
  <p>$\mathbf{·}$ 表示内积（inner product）， $\otimes$ 表示外积（tensor product）。</p>
</blockquote>

<p>最终我们获得了一个 $V \times N$ 的矩阵，因为  $\mathbf{x}$ 中只有一个元素不为 0，所以，在 $ \dfrac{\partial E}{\partial \mathbf{W}} $ 中只有一行为非零行，这一行即为 $\mathrm{EH}^{\mathrm{T}}$，是一个 $n$ 维行向量。最终的更新公式为：</p>

<script type="math/tex; mode=display">\mathbf{v}_{w_{I}}^{(\text{new})}=\mathbf{v}_{w_{I}}^{(\text{old})}-\eta \cdot \mathrm{EH}^{\mathrm{T}} \tag{16}</script>

<p>其中，$\mathbf{v}<em>{w_I}$ 是 $\mathbf{W}$ 的行向量，所以 $\mathbf{v}</em>{w_I}^{\mathrm{T}}$ 也是上下文词的输入词向量，在上下文词只有一个的情况下，在一次迭代中这是唯一导数不为 0 的行。$\mathbf{W}$ 的其他行在这次迭代中保持不变，因为他们的导数都是 0。</p>

<h2 id="12-multi-word-context">1.2 Multi-word context</h2>

<p>下面我们扩展一下上面介绍的模型结构，如下图所示：</p>

<p><img src="/img/in-post/post-cs224n/CBOW_arc.png" alt="" /></p>

<p>这里我们认为有多个上下文词，假设有 $C$ 个，来预测一个目标词（即中心词）。和 1.1 中介绍的类似，我们将每个上下文词与 $\mathbf{W}^{\mathrm{T}}$ 做矩阵乘法，然后对这 $C$ 个结果取平均，作为中间层的隐藏向量。</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} \mathbf{h} &= \dfrac{1}{C} \mathbf{W}^{\mathrm{T}} (\mathbf{x}_1+\mathbf{x}_2+ \cdots + \mathbf{x}_C) \tag{17} \\ &= \dfrac{1}{C} (\mathbf{v}_{w_{1}}+\mathbf{v}_{w_{2}}+\cdots+\mathbf{v}_{w_{C}})^{\mathrm{T}}  \tag{18}\end{align} %]]></script>

<p>同样这里的 $\mathbf{v}_{w}$ 是输入词向量，$w_1, w_2, …, w_C$ 是上下文词，所以损失函数为：</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} E&=-\log p\left(w_{O} \mid w_{I,1},w_{I,2},\cdots,w_{I,C}\right)\tag{19}\\ &=-u_{j^{*}} + \log \sum_{j^{\prime}=1}^{V} \exp \left(u_{j^{\prime}}\right)\tag{20}\\ &= -\mathbf{v}_{w_{O}}^{\prime \mathrm{T}} \mathbf{h} + \log \sum_{j^{\prime}=1}^{V} \exp \left(\mathbf{v}_{w_{j}}^{\prime \mathrm{T}} \mathbf{h}\right)\tag{21}\end{align} %]]></script>

<p>式（21）与式（7）本质相同，因为今后我们基本不会遇到上下文词只有一个的情况，所以，CBOW 的损失函数可确定为式（21）。</p>

<h3 id="121-更新-mathbfwprime">1.2.1 更新 $\mathbf{W}^{\prime}$</h3>

<p>与 1.1.1 中的推导过程相同，从隐藏层到输出层的权重更新式为：</p>

<script type="math/tex; mode=display">\mathbf{v}_{w_{j}}^{\prime \; (\text{new})}=\mathbf{v}_{w_{j}}^{\prime \; (\text{old})}-\eta \cdot e_{j} \cdot \mathbf{h} \quad \text { for } j=1,2, \cdots, V \tag{22}</script>

<p>注意，对于每一个训练实例，都需要应用这个更新式来更新 $\mathbf{W}^{\prime}$ 中的每一个元素。</p>

<h3 id="122-更新-mathbfw">1.2.2 更新 $\mathbf{W}$</h3>

<p>与 1.1.2 中的推导过程稍微有些不同，从隐藏层到输出层的权重更新式为：</p>

<script type="math/tex; mode=display">\mathbf{v}_{w_{I,c}}^{(\text{new})}=\mathbf{v}_{w_{I,c}}^{(\text{old})}-\dfrac{1}{C}\cdot\eta \cdot \mathrm{EH}^{\mathrm{T}} \quad \text { for } c=1,2, \cdots, C \tag{23}</script>

<p>简单地推导一下：</p>

<p>$\text{for  c = 1, 2,…, C}:$</p>

<script type="math/tex; mode=display">\dfrac{\partial E}{\partial h_{i}}=\sum_{j=1}^{V} \dfrac{\partial E}{\partial u_{j}} \cdot \dfrac{\partial u_{j}}{\partial h_{i}}=\sum_{j=1}^{V} e_{j} \cdot w_{i j}^{\prime} :=\mathrm{EH}_{i}</script>

<script type="math/tex; mode=display">h_{i}=\dfrac{1}{C} \sum_{c=1}^C\sum_{k=1}^{V} x_{ck} \cdot w_{k i}^{c}</script>

<blockquote>
  <p>$h_i$  的表达式与 1.1.2 中的不同</p>
</blockquote>

<script type="math/tex; mode=display">\dfrac{\partial E}{\partial w_{k i}^c}=\dfrac{\partial E}{\partial h_{i}} \cdot \dfrac{\partial h_{i}}{\partial w_{k i}^c}=\mathrm{EH}_{i} \cdot \dfrac{1}{C} \cdot x_{ck}</script>

<script type="math/tex; mode=display">\dfrac{\partial E}{\partial \mathbf{W}^c}=\dfrac{1}{C}\mathbf{x}_c \otimes \mathbf{E H}=\dfrac{1}{C}\mathbf{x}_c \mathrm{EH}^{\mathrm{T}}</script>

<p>最终我们获得了 $C$ 个 $V \times N$ 的矩阵，因为  $\mathbf{x}$ 中只有一个元素不为 0，所以，在 $ \dfrac{\partial E}{\partial \mathbf{W}^c} $ 中只有一行为非零行，这一行即为 $\mathrm{EH}^{\mathrm{T}}$，是一个 $n$ 维行向量。</p>

<p>容易看出，从输入层到隐藏层的权重更新式与式（16）相似，只不过我们需要将式（23）应用于更新每一个上下文词 $w_{I,c}$。式（23）中，$\mathbf{v}<em>{w</em>{I,c}}$ 表示第 $c$ 个输入上下文词中的词向量，$\eta$ 为正的学习率，$\mathrm{EH} = \dfrac{\partial E}{\partial h_{i}}$ 由式（12）给出。其中，$\mathbf{v}<em>{w_I,c}$ 是 $\mathbf{W^c}$ 的行向量，所以 $\mathbf{v}</em>{w_{I,c}}^{\mathrm{T}}$也是上下文词的输入词向量，在上下文词只有一个的情况下，在一次迭代中这是唯一导数不为 0 的行。$\mathbf{W^c}$ 的其他行在这次迭代中保持不变，因为他们的导数都是 0。</p>

<h2 id="2-skip-gram-model">2 Skip-Gram Model</h2>

<p>Skip-Gram 与 CBOW 相反，输入是中心词，输出是上下文词。</p>

<blockquote>
  <p>Skip-Gram 是 一 预测 多。</p>

  <p>CBOW 是 多 预测 一。</p>
</blockquote>

<p><img src="/img/in-post/post-cs224n/SG_arc.png" alt="" /></p>

<p>我们仍然使用 $\mathbf{v}_{w_I}$ 表示输入层中心词的输入词向量，因此隐藏层状态 $h$ 的定义与式（1）相同。也即意味着 $h$ 只是复制（转置）输入层 $\to$ 隐藏层 权重矩阵 $\mathbf{W}$ 的一行，</p>

<script type="math/tex; mode=display">\mathbf{h}=\mathbf{W}^{\mathrm{T}} \mathbf{x}=\mathbf{W}_{(k, \cdot)}^{ \quad \mathrm{T}} :=\mathbf{v}_{w_{I}}^{\mathrm{T}}  \tag{24}</script>

<p>在输出层，我们将输出 $C$ 个概率分布，而不是一个。 使用相同的 隐藏层 $\to$ 输出层 矩阵计算每个输出：</p>

<script type="math/tex; mode=display">p\left(w_{c,j}=w_{O,c} \mid w_{I}\right)=y_{c, j}= softmax(u_{c,j})=\dfrac{\exp \left(u_{c,j}\right)}{\sum_{j^{\prime}=1}^{V} \exp \left(u_{j^{\prime}}\right)}  \tag{25}</script>

<p>其中，$w_{c,j}$ 是第 $c$ 个 $\mathbf{W}^{\prime}$ 中的第 $j$ 列的输出词向量所代表的词； $w_{O,c}$ 输出上下文词中的第 $c$ 个词； $w_{I}$ 是输入词； $y_{c,j}$ 是第 $c$ 个 $\mathbf{W}^{\prime}$ 中的第 $j$ 个单元的输出； $u_{c,j}$ 第 $c$ 个 $\mathbf{W}^{\prime}$ 的输出中第 $j$ 个分数；因为输出权重矩阵共享参数，因此：</p>

<script type="math/tex; mode=display">u_{c,j}=u_{j}=\mathbf{v}_{w_{j}}^{\prime \quad \mathrm{T}} \cdot \mathbf{h,} \quad \text{for c = 1, 2, ... , C} \tag{26}</script>

<p>其中，$\mathbf{v}<em>{w</em>{j}}^{\prime}$ 是第 $j$ 个输出词向量对应的词，也是 $\mathbf{W}^{\prime}$ 第 $j$ 列。</p>

<blockquote>
  <p>这里提一句，模型初始化时，生成相同的 C 个分布，但是在反向传播时，由于每个词不同，各自减去的比例也不同，因此 $\mathbf{W}^{\prime}$ 也会根据不同的词发生不同的变化。</p>
</blockquote>

<h2 id="21-更新-mathbfwprime">2.1 更新 $\mathbf{W}^{\prime}$</h2>

<p>隐藏层 $\to$ 输出层的参数更新方程的推导与 1.1 中的没有太大不同。损失函数改为：</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} E&=-\log p\left(w_{O} \mid w_{I,1},w_{I,2},\cdots,w_{I,C}\right)\tag{27}\\ &= -\log[p\left(w_{O,1} \mid w_{I}  \right) p\left(w_{O,2} \mid w_{I} \right) \cdots p\left(w_{O,C} \mid w_{I}  \right)] \tag{28} \\ &=-\log \prod_{c=1}^{C} \frac{\exp \left(u_{c, j_{c}^{*}}\right)}{\sum_{j^{\prime}=1}^{V} \exp \left(u_{j^{\prime}}\right)}\tag{29}\\ &= -\log [\prod_{c=1}^{C} \exp \left(u_{c, j_{c}^{*}}\right)] + -\log [\prod_{c=1}^{C} \sum_{j^{\prime}=1}^{V} \exp \left(u_{ j^{\prime}}\right)] \tag{30} \\ &= -\sum_{c=1}^{C} u_{j_{c}^{*}}+C \cdot \log \sum_{j^{\prime}=1}^{V} \exp \left(u_{j^{\prime}}\right)\tag{31}\end{align} %]]></script>

<p>其中， $u_{j_{c}^{*}}$ 是第 $c$ 个输出上下文词的索引。</p>

<p>我们求输出层的每个 panel 上的每个单元的输入 $u_{c,j}$ 对 $E$ 的偏导数，得到：</p>

<script type="math/tex; mode=display">\dfrac{\partial E}{\partial u_{c, j}}=y_{c, j}-t_{c, j}:=e_{c, j} \tag{32}</script>

<blockquote>
  <p>这里的 panel 的意思是 “面板” ， 可以看 Skip-Gram 模型的输出部分加以理解。</p>
</blockquote>

<p>和（8）类似， $e_{c, j}$ 是预测误差。为了简化标记，我们定义一个 $V$ 维的向量 $\mathrm{EI}=\left{\mathrm{EI}<em>{1}, \cdots, \mathrm{EI}</em>{V}\right}$ ，作为所有输出上下文词的预测误差的和。</p>

<script type="math/tex; mode=display">\mathrm{EI}_{j}=\sum_{c=1}^{C} e_{c, j} \tag{33}</script>

<p>然后，我们求 $\mathbf{W}^{\prime}$ 对 $E$ 的偏导数：</p>

<script type="math/tex; mode=display">\dfrac{\partial E}{\partial w_{i j}^{\prime}}=\sum_{c=1}^{C} \dfrac{\partial E}{\partial u_{c, j}} \cdot \dfrac{\partial u_{c, j}}{\partial w_{i j}^{\prime}}=\mathrm{EI}_{j} \cdot h_{i} \tag{34}</script>

<p>最终得到 $\mathbf{W}^{\prime}$ 的更新式：</p>

<script type="math/tex; mode=display">w_{i j}^{\prime \quad \mathrm{(new)}} = w_{i j}^{\prime \quad \mathrm{(old)}}-\eta \cdot \mathrm{EI}_{j} \cdot h_{i} \tag{35}</script>

<p>或，</p>

<script type="math/tex; mode=display">\mathbf{v}_{w_{j}}^{\prime \quad \mathrm{(new)}} =\mathbf{v}_{w_{j}}^{\prime \quad \mathrm{(old)}}-\eta \cdot \mathrm{EI}_{j} \cdot \mathbf{h} \quad \text { for } j=1,2, \cdots, V \tag{36}</script>

<p>除了预测误差是输出层中所有上下文词中累加而成的外，对更新方程（36）的直观理解与（11）相同。 注意，我们需要为每个训练实例的 hidden $\to$ output 矩阵的每个元素应用此更新方程。</p>

<h2 id="22-更新-mathbfw">2.2 更新 $\mathbf{W}$</h2>

<p>输入层 $\to$ 隐藏层矩阵的更新公式的推导与（12）至（16）相同，不同之处在于将预测误差 $e_j$ 替换为 $\mathrm{EI}_j$ 。 我们直接给出更新公式：</p>

<script type="math/tex; mode=display">\mathbf{v}_{w_{I}}^{(\text {new})}=\mathbf{v}_{w_{I}}^{(\text {old})}-\eta \cdot \mathrm{EH}^{\mathrm{T}} \tag{37}</script>

<p>其中 $\mathrm{EH}$ 是一个 $N$ 维向量。由（12）：</p>

<script type="math/tex; mode=display">\dfrac{\partial E}{\partial h_{i}}=\sum_{j=1}^{V} \sum_{c=1}^{C} \dfrac{\partial E}{\partial u_{c, j}} \cdot \dfrac{\partial u_{c, j}}{\partial h_{i}} = \sum_{j=1}^{V} \underbrace{\sum_{c=1}^{C} e_{c,j}}_{\mathrm{EI}_{j}} \cdot w_{ij}^{\prime} = \mathrm{EH}_i \tag{38}</script>

<p>所以 $\mathrm{EH}$ 的每个元素为：</p>

<script type="math/tex; mode=display">\mathrm{EH}_i = \sum_{j=1}^{V} e_{c,j} \cdot w_{ij}^{\prime} \tag{39}</script>

<p>对（37）的理解与（16）处的相同。</p>

<h1 id="3-小结">3 小结</h1>

<p>这篇文章论述了 word2vec 的两个模型 CBOW 和 Skip-Gram 的数学表示，包含详细的数学推导及验证，如果有难以理解的地方，请一定要举出具体的例子，比如，举出一个 $\mathbf{W}$ 和 $\mathbf{W}^{\prime}$ 去走一遍数学推导。由于篇幅不宜过长，两种可以提高训练效率的方法将在下一篇文章中详细说明。</p>

<h2 id="参考文献">参考文献</h2>

<p><a href="https://arxiv.org/pdf/1411.2738.pdf">word2vec Parameter Learning Explained </a></p>


                <hr style="visibility: hidden;">
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2019/08/22/NLP_algorithm_engineer_attacking/" data-toggle="tooltip" data-placement="top" title="NLP算法工程师进击之路">
                        Previous<br>
                        <span>NLP算法工程师进击之路</span>
                        </a>
                    </li>
                    
                    
                </ul>
                <hr style="visibility: hidden;">

                

                
            </div>  

    <!-- Side Catalog Container -->
        
            <div class="
                col-lg-2 col-lg-offset-0
                visible-lg-block
                sidebar-container
                catalog-container">
                <div class="side-catalog">
                    <hr class="hidden-sm hidden-xs">
                    <h5>
                        <a class="catalog-toggle" href="#">CATALOG</a>
                    </h5>
                    <ul class="catalog-body"></ul>
                </div>
            </div>
        

    <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                


<section>
    
        <hr class="hidden-sm hidden-xs">
    
    <h5><a href="/archive/">FEATURED TAGS</a></h5>
    <div class="tags">
        
        
        
        </a>
        
        
                <a data-sort="0002" 
                    href="/archive/?tag=NLP"
                    title="NLP"
                    rel="4">NLP</a>
        
                <a data-sort="0002" 
                    href="/archive/?tag=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"
                    title="深度学习"
                    rel="4">深度学习</a>
        
                <a data-sort="0003" 
                    href="/archive/?tag=Stanford"
                    title="Stanford"
                    rel="3">Stanford</a>
        
                <a data-sort="0004" 
                    href="/archive/?tag=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"
                    title="机器学习"
                    rel="2">机器学习
    </div>
</section>


                <!-- Friends Blog -->
                
            </div>
        </div>
    </div>
</article>

<!-- add support for mathjax by voleking-->

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    SVG: {
      scale: 90
    },
    tex2jax: {
      inlineMath: [ ['$','$'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true,
    }
  });
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG">
</script>









<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'always',
          placement: 'right',
          icon: '#'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <!-- SNS Link -->
                


<ul class="list-inline text-center">


  
  
  
  <li>
    <a target="_blank" href="https://www.zhihu.com/people/陈三天">
      <span class="fa-stack fa-lg">
        <i class="fa fa-circle fa-stack-2x"></i>
        <i class="fa  fa-stack-1x fa-inverse">知</i>
      </span>
    </a>
  </li>
  
  
  <li>
    <a target="_blank" href="http://weibo.com/Kgoeson">
      <span class="fa-stack fa-lg">
        <i class="fa fa-circle fa-stack-2x"></i>
        <i class="fa fa-weibo fa-stack-1x fa-inverse"></i>
      </span>
    </a>
  </li>
  
  
  
  <li>
    <a target="_blank" href="https://github.com/Kgoeson">
      <span class="fa-stack fa-lg">
        <i class="fa fa-circle fa-stack-2x"></i>
        <i class="fa fa-github fa-stack-1x fa-inverse"></i>
      </span>
    </a>
  </li>
  
  
</ul>

                <p class="copyright text-muted">
                    Copyright &copy; ChenJing's Blog 2019
                    <br>
                    Powered by <a href="http://huangxuan.me">Hux Blog</a> |
                    <iframe
                        style="margin-left: 2px; margin-bottom:-5px;"
                        frameborder="0" scrolling="0" width="100px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=huxpro&repo=huxpro.github.io&type=star&count=true" >
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js "></script>

<!-- Bootstrap Core JavaScript -->
<!-- Currently, only navbar scroll-down effect at desktop still depends on this -->
<script src="/js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js "></script>

<!-- Service Worker -->

<script src="/js/snackbar.js "></script>
<script src="/js/sw-registration.js "></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!--
     Because of the native support for backtick-style fenced code blocks
     right within the Markdown is landed in Github Pages,
     From V1.6, There is no need for Highlight.js,
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/
     - https://github.com/jneen/rouge/wiki/list-of-supported-languages-and-lexers
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->





<!--fastClick.js -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->

<script>
    // dynamic User by Hux
    var _gaId = 'UA-145017913-1';
    var _gaDomain = 'auto';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>



<!-- Baidu Tongji -->



<!-- Side Catalog -->

<script type="text/javascript">
    function generateCatalog (selector) {

        // interop with multilangual 
        if ('' == 'true') {
            _containerSelector = 'div.post-container.active'
        } else {
            _containerSelector = 'div.post-container'
        }

        // init
        var P = $(_containerSelector),a,n,t,l,i,c;
        a = P.find('h1,h2,h3,h4,h5,h6');

        // clean
        $(selector).html('')

        // appending
        a.each(function () {
            n = $(this).prop('tagName').toLowerCase();
            i = "#"+$(this).prop('id');
            t = $(this).text();
            c = $('<a href="'+i+'" rel="nofollow">'+t+'</a>');
            l = $('<li class="'+n+'_nav"></li>').append(c);
            $(selector).append(l);
        });
        return true;
    }

    generateCatalog(".catalog-body");

    // toggle side catalog
    $(".catalog-toggle").click((function(e){
        e.preventDefault();
        $('.side-catalog').toggleClass("fold")
    }))

    /*
     * Doc: https://github.com/davist11/jQuery-One-Page-Nav
     * Fork by Hux to support padding
     */
    async("/js/jquery.nav.js", function () {
        $('.catalog-body').onePageNav({
            currentClass: "active",
            changeHash: !1,
            easing: "swing",
            filter: "",
            scrollSpeed: 700,
            scrollOffset: 0,
            scrollThreshold: .2,
            begin: null,
            end: null,
            scrollChange: null,
            padding: 80
        });
    });
</script>



<!-- Multi-Lingual -->




<!-- Image to hack wechat -->
<img src="/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
